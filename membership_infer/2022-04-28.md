### Title: The Duality of Networks and Foci: Generative Models of Two-Mode Networks from One-Mode Networks
* Paper ID: 2204.13670v1
* Paper URL: [http://arxiv.org/abs/2204.13670v1](http://arxiv.org/abs/2204.13670v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: Focus theory describes how shared social statuses, beliefs, and places (i.e.,
foci) can facilitate the formation of social ties, while two-mode projections
provide a method for transforming two-mode data on individuals' memberships in
foci into a one-mode network of their co-memberships. In this paper, I explore
the opposite process: how social ties can facilitate the formation of foci, and
how two-mode data can be generated from a one-mode network. Drawing on theories
of team, social group, and organization recruitment, I propose three models
that describe how such foci might form from the relationships in a social
network. I show that these models can be used to generate empirically plausible
two-mode networks characterized by positively-skewed degree distributions and
an over-representation of four- and six-cycles. I conclude by discussing these
models' limitations, and highlighting how they might be used to study two-mode
networks representing social foci, and to investigate two-mode projection
methods.

### Title: Bernstein - von Mises theorem and misspecified models: a review
* Paper ID: 2204.13614v1
* Paper URL: [http://arxiv.org/abs/2204.13614v1](http://arxiv.org/abs/2204.13614v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: This is a review of asymptotic and non-asymptotic behaviour of Bayesian
methods under model specification. In particular we focus on consistency, i.e.
convergence of the posterior distribution to the point mass at the best
parametric approximation to the true model, and conditions for it to be locally
Gaussian around this point. For well specified regular models, variance of the
Gaussian approximation coincides with the Fisher information, making Bayesian
inference asymptotically efficient. In this review, we discuss how this is
affected by model misspecification. We also discuss approaches to adjust
Bayesian inference to make it asymptotically efficient under model
misspecification.

### Title: Multi-Mask Least-Squares Deconvolution: Extracting RVs using tailored masks
* Paper ID: 2204.13556v1
* Paper URL: [http://arxiv.org/abs/2204.13556v1](http://arxiv.org/abs/2204.13556v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: To push the radial velocity (RV) exoplanet detection threshold, it is crucial
to find more reliable radial velocity extraction methods. The Least-Squares
Deconvolution (LSD) technique has been used to infer the stellar magnetic flux
from spectropolarimetric data for the past two decades. It relies on the
assumption that stellar absorption lines are similar in shape. Although this
assumption is simplistic, LSD provides a good model for intensity spectra and
likewise an estimate for their Doppler shift. We present the Multi-Mask
Least-Squares Deconvolution (MM-LSD) RV extraction pipeline which extracts the
radial velocity from two-dimensional echelle-order spectra using LSD with
multiple tailored masks after continuum normalisation and telluric absorption
line correction. The flexibility of LSD allows to exclude spectral lines or
pixels at will, providing a means to exclude variable lines or pixels affected
by instrumental problems. The MM-LSD pipeline was tested on HARPS-N data for
the Sun and selected well-observed stars with 5.7 < Vmag < 12.6. For FGK-type
stars with median signal-to-noise above 100, the pipeline delivered RV time
series with on average 12 per cent lower scatter as compared to the HARPS-N RV
extraction pipeline based on the Cross-Correlation Function technique. The
MM-LSD pipeline may be used as a standalone RV code, or modified and extended
to extract a proxy for the magnetic field strength.

### Title: Tragedy Plus Time: Capturing Unintended Human Activities from Weakly-labeled Videos
* Paper ID: 2204.13548v1
* Paper URL: [http://arxiv.org/abs/2204.13548v1](http://arxiv.org/abs/2204.13548v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: In videos that contain actions performed unintentionally, agents do not
achieve their desired goals. In such videos, it is challenging for computer
vision systems to understand high-level concepts such as goal-directed
behavior, an ability present in humans from a very early age. Inculcating this
ability in artificially intelligent agents would make them better social
learners by allowing them to evaluate human action under a teleological lens.
To validate the ability of deep learning models to perform this task, we curate
the W-Oops dataset, built upon the Oops dataset [15]. W-Oops consists of 2,100
unintentional human action videos, with 44 goal-directed and 30 unintentional
video-level activity labels collected through human annotations. Due to the
expensive segment annotation procedure, we propose a weakly supervised
algorithm for localizing the goal-directed as well as unintentional temporal
regions in the video leveraging solely video-level labels. In particular, we
employ an attention mechanism-based strategy that predicts the temporal regions
which contribute the most to a classification task. Meanwhile, our designed
overlap regularization allows the model to focus on distinct portions of the
video for inferring the goal-directed and unintentional activity while
guaranteeing their temporal ordering. Extensive quantitative experiments verify
the validity of our localization method. We further conduct a video captioning
experiment which demonstrates that the proposed localization module does indeed
assist teleological action understanding.

### Title: Streaming Multiscale Deep Equilibrium Models
* Paper ID: 2204.13492v1
* Paper URL: [http://arxiv.org/abs/2204.13492v1](http://arxiv.org/abs/2204.13492v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: We present StreamDEQ, a method that infers frame-wise representations on
videos with minimal per-frame computation. In contrast to conventional methods
where compute time grows at least linearly with the network depth, we aim to
update the representations in a continuous manner. For this purpose, we
leverage the recently emerging implicit layer model which infers the
representation of an image by solving a fixed-point problem. Our main insight
is to leverage the slowly changing nature of videos and use the previous frame
representation as an initial condition on each frame. This scheme effectively
recycles the recent inference computations and greatly reduces the needed
processing time. Through extensive experimental analysis, we show that
StreamDEQ is able to recover near-optimal representations in a few frames time,
and maintain an up-to-date representation throughout the video duration. Our
experiments on video semantic segmentation and video object detection show that
StreamDEQ achieves on par accuracy with the baseline (standard MDEQ) while
being more than $3\times$ faster. The project page is available at:
https://ufukertenli.github.io/streamdeq/

### Title: Mahalanobis balancing: a multivariate perspective on approximate covariate balancing
* Paper ID: 2204.13439v1
* Paper URL: [http://arxiv.org/abs/2204.13439v1](http://arxiv.org/abs/2204.13439v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: In the past decade, various exact balancing-based weighting methods were
introduced to the causal inference literature. Exact balancing alleviates the
extreme weight and model misspecification issues that may incur when one
implements inverse probability weighting. It eliminates covariate imbalance by
imposing balancing constraints in an optimization problem. The optimization
problem can nevertheless be infeasible when there is bad overlap between the
covariate distributions in the treated and control groups or when the
covariates are high-dimensional. Recently, approximate balancing was proposed
as an alternative balancing framework, which resolves the feasibility issue by
using inequality moment constraints instead. However, it can be difficult to
select the threshold parameters when the number of constraints is large.
Moreover, moment constraints may not fully capture the discrepancy of covariate
distributions. In this paper, we propose Mahalanobis balancing, which
approximately balances covariate distributions from a multivariate perspective.
We use a quadratic constraint to control overall imbalance with a single
threshold parameter, which can be tuned by a simple selection procedure. We
show that the dual problem of Mahalanobis balancing is an l_2 norm-based
regularized regression problem, and establish interesting connection to
propensity score models. We further generalize Mahalanobis balancing to the
high-dimensional scenario. We derive asymptotic properties and make extensive
comparisons with existing balancing methods in the numerical studies.

### Title: Known unknowns: assessing the impact of instrumental calibration uncertainty on LISA science
* Paper ID: 2204.13405v1
* Paper URL: [http://arxiv.org/abs/2204.13405v1](http://arxiv.org/abs/2204.13405v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: The primary scientific results of the future space-based gravitational wave
interferometer LISA will come from the parameter inference of a large variety
of gravitational wave sources. However, the presence of calibration errors
could potentially degrade the measurement precision of the system parameters.
Here, we assess the impact of calibration uncertainties on parameter estimation
for individual sources, focusing on massive black holes, extreme-mass-ratio
inspirals (EMRIs), galactic binaries, and stellar origin black hole binaries.
Using a Fisher matrix formalism, we investigate how the measurement precision
of source parameters degrades as a function of the size of the assumed
calibration uncertainties. If we require that parameter measurements are
degraded by no more than a factor of two relative to their value in the absence
of calibration error, we find that calibration errors should be smaller than a
few tenths of a percent in amplitude and $10^{-3}$ in phase. We also
investigate the possibility of using verification binaries and EMRIs to
constrain calibration uncertainties. Verification binaries can constrain
amplitude calibration uncertainties at the level of a few percent, while both
source types can provide constrain phase calibration at the level of a
few$\times10^{-2}$.

### Title: Theoretical analysis of the extraction of neutron skin thickness from coherent Ï€0 photoproduction off nuclei
* Paper ID: 2204.13395v1
* Paper URL: [http://arxiv.org/abs/2204.13395v1](http://arxiv.org/abs/2204.13395v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: Background: Coherent {\pi}0 photoproduction on heavy nuclei has been
suggested as a reliable tool to infer neutron skin thicknesses. To this aim,
various experiments have been performed, especially on 208Pb. Purpose: We
analyze the sensitivity of that reaction process to the nucleonic density, and
especially to the neutron skin thickness, for 12C, 40Ca and 208Pb, for which
reliable data exist, and on 116,124Sn, for which measurements have been
performed in Mainz. We study also the role played by the {\pi}0-nucleus
final-state interaction. Method: A model of the reaction is developed at the
impulse approximation considering either plane waves or distorted waves to
describe the {\pi}0-nucleus scattering in the outgoing channel. Results: Our
calculations are in good agreement with existing data, especially for 208Pb.
The sensitivity of the theoretical cross sections to the choice of the
nucleonic density is small, and below the experimental resolution. Conclusions:
Coherent {\pi}0 photoproduction is mostly an isoscalar observable that bares no
practical sensitivity to the neutron skin thickness. To infer that structure
observable it should be coupled to other reaction measurements, such as
electron scattering, or by comparing experiments performed on isotopes of the
same chemical element.

### Title: Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation
* Paper ID: 2204.13362v1
* Paper URL: [http://arxiv.org/abs/2204.13362v1](http://arxiv.org/abs/2204.13362v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: Attribute-based Controlled Text Generation (CTG) refers to generating
sentences that satisfy desirable attributes (e.g., emotions and topics).
Existing works often utilize fine-tuning or resort to extra attribute
classifiers, yet suffer from storage and inference time increases. To address
these concerns, we explore attribute-based CTG in a prompt-based manner. In
short, the proposed Tailor represents each attribute as a pre-trained
continuous vector (i.e., single-attribute prompt) and guides the generation of
a fixed PLM switch to a pre-specified attribute. We experimentally find that
these prompts can be simply concatenated as a whole to multi-attribute CTG
without any re-training, yet raises problems of fluency decrease and position
sensitivity. To this end, Tailor provides a multi-attribute prompt mask and a
re-indexing position-ids sequence to bridge the gap between the training (one
prompt for each task) and testing stage (concatenating more than one prompt).
To further enhance such single-attribute prompt combinations, Tailor also
introduces a trainable prompt connector, which can be concatenated with any two
single-attribute prompts to multi-attribute text generation. Experiments on 11
attribute-specific generation tasks demonstrate strong performances of Tailor
on both single-attribute and multi-attribute CTG, with 0.08\% training
parameters of a GPT-2.

### Title: It's DONE: Direct ONE-shot learning without training optimization
* Paper ID: 2204.13361v1
* Paper URL: [http://arxiv.org/abs/2204.13361v1](http://arxiv.org/abs/2204.13361v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: Learning a new concept from one example is a superior function of human brain
and it is drawing attention in the field of machine learning as one-shot
learning task. In this paper, we propose the simplest method for this task,
named Direct ONE-shot learning (DONE). DONE adds a new class to a pretrained
deep neural network (DNN) classifier with neither training optimization nor
other-classes modification. DONE is inspired by Hebbian theory and directly
uses the neural activity input of the final dense layer obtained from a data
that belongs to the new additional class as the connectivity weight (synaptic
strength) with a newly-provided-output neuron for the new class. DONE requires
just one inference for obtaining the output of the final dense layer and its
procedure is simple, deterministic, not requiring parameter tuning and
hyperparameters. The performance of DONE depends entirely on the pretrained DNN
model used as a backbone model, and we confirmed that DONE with a well-trained
backbone model performs a practical-level accuracy. DONE has some advantages
including a DNN's practical use that is difficult to spend high cost for a
training, an evaluation of existing DNN models, and the understanding of the
brain. DONE might be telling us one-shot learning is an easy task that can be
achieved by a simple principle not only for humans but also for current
well-trained DNN models.

### Title: A Closer Look at Branch Classifiers of Multi-exit Architectures
* Paper ID: 2204.13347v1
* Paper URL: [http://arxiv.org/abs/2204.13347v1](http://arxiv.org/abs/2204.13347v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: Multi-exit architectures consist of a backbone and branch classifiers that
offer shortened inference pathways to reduce the run-time of deep neural
networks. In this paper, we analyze different branching patterns that vary in
their allocation of computational complexity for the branch classifiers.
Constant-complexity branching keeps all branches the same, while
complexity-increasing and complexity-decreasing branching place more complex
branches later or earlier in the backbone respectively. Through extensive
experimentation on multiple backbones and datasets, we find that
complexity-decreasing branches are more effective than constant-complexity or
complexity-increasing branches, which achieve the best accuracy-cost trade-off.
We investigate a cause by using knowledge consistency to probe the effect of
adding branches onto a backbone. Our findings show that complexity-decreasing
branching yields the least disruption to the feature abstraction hierarchy of
the backbone, which explains the effectiveness of the branching patterns.

### Title: Temporal Progressive Attention for Early Action Prediction
* Paper ID: 2204.13340v1
* Paper URL: [http://arxiv.org/abs/2204.13340v1](http://arxiv.org/abs/2204.13340v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: Early action prediction deals with inferring the ongoing action from
partially-observed videos, typically at the outset of the video. We propose a
bottleneck-based attention model that captures the evolution of the action,
through progressive sampling over fine-to-coarse scales. Our proposed Temporal
Progressive (TemPr) model is composed of multiple attention towers, one for
each scale. The predicted action label is based on the collective agreement
considering confidences of these attention towers. Extensive experiments over
three video datasets showcase state-of-the-art performance on the task of Early
Action Prediction across a range of backbone architectures. We demonstrate the
effectiveness and consistency of TemPr through detailed ablations.

### Title: Towards Flexible Inference in Sequential Decision Problems via Bidirectional Transformers
* Paper ID: 2204.13326v1
* Paper URL: [http://arxiv.org/abs/2204.13326v1](http://arxiv.org/abs/2204.13326v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: Randomly masking and predicting word tokens has been a successful approach in
pre-training language models for a variety of downstream tasks. In this work,
we observe that the same idea also applies naturally to sequential decision
making, where many well-studied tasks like behavior cloning, offline RL,
inverse dynamics, and waypoint conditioning correspond to different sequence
maskings over a sequence of states, actions, and returns. We introduce the
FlexiBiT framework, which provides a unified way to specify models which can be
trained on many different sequential decision making tasks. We show that a
single FlexiBiT model is simultaneously capable of carrying out many tasks with
performance similar to or better than specialized models. Additionally, we show
that performance can be further improved by fine-tuning our general model on
specific tasks of interest.

### Title: MMRotate: A Rotated Object Detection Benchmark using Pytorch
* Paper ID: 2204.13317v1
* Paper URL: [http://arxiv.org/abs/2204.13317v1](http://arxiv.org/abs/2204.13317v1)
* Updated Date: 2022-04-28
* Code URL: [https://github.com/open-mmlab/mmrotate](https://github.com/open-mmlab/mmrotate)
* Summary: We present an open-source toolbox, named MMRotate, which provides a coherent
algorithm framework of training, inferring, and evaluation for the popular
rotated object detection algorithm based on deep learning. MMRotate implements
18 state-of-the-art algorithms and supports the three most frequently used
angle definition methods. To facilitate future research and industrial
applications of rotated object detection-related problems, we also provide a
large number of trained models and detailed benchmarks to give insights into
the performance of rotated object detection. MMRotate is publicly released at
https://github.com/open-mmlab/mmrotate.

### Title: Efficient and Accurate Conversion of Spiking Neural Network with Burst Spikes
* Paper ID: 2204.13271v1
* Paper URL: [http://arxiv.org/abs/2204.13271v1](http://arxiv.org/abs/2204.13271v1)
* Updated Date: 2022-04-28
* Code URL: [https://github.com/brain-inspired-cognitive-engine/conversion_burst](https://github.com/brain-inspired-cognitive-engine/conversion_burst)
* Summary: Spiking neural network (SNN), as a brain-inspired energy-efficient neural
network, has attracted the interest of researchers. While the training of
spiking neural networks is still an open problem. One effective way is to map
the weight of trained ANN to SNN to achieve high reasoning ability. However,
the converted spiking neural network often suffers from performance degradation
and a considerable time delay. To speed up the inference process and obtain
higher accuracy, we theoretically analyze the errors in the conversion process
from three perspectives: the differences between IF and ReLU, time dimension,
and pooling operation. We propose a neuron model for releasing burst spikes, a
cheap but highly efficient method to solve residual information. In addition,
Lateral Inhibition Pooling (LIPooling) is proposed to solve the inaccuracy
problem caused by MaxPooling in the conversion process. Experimental results on
CIFAR and ImageNet demonstrate that our algorithm is efficient and accurate.
For example, our method can ensure nearly lossless conversion of SNN and only
use about 1/10 (less than 100) simulation time under 0.693$\times$ energy
consumption of the typical method. Our code is available at
https://github.com/Brain-Inspired-Cognitive-Engine/Conversion_Burst.

### Title: GWCloud: a searchable repository for the creation and curation of gravitational-wave inference results
* Paper ID: 2204.13267v1
* Paper URL: [http://arxiv.org/abs/2204.13267v1](http://arxiv.org/abs/2204.13267v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: There are at present ${\cal O}(100)$ gravitational-wave candidates from
compact binary mergers reported in the astronomical literature. As detector
sensitivities are improved, the catalog will swell in size: first to ${\cal
O}(1000)$ events in the A+ era and then to ${\cal O}(10^6)$ events in the era
of third-generation observatories like Cosmic Explorer and the Einstein
Telescope. Each event is analyzed using Bayesian inference to determine
properties of the source including component masses, spins, tidal parameters,
and the distance to the source. These inference products are the fodder for
some of the most exciting gravitational-wave science, enabling us to measure
the expansion of the Universe with standard sirens, to characterise the neutron
star equation of state, and to unveil how and where gravitational-wave sources
are assembled. In order to maximize the science from the coming deluge of
detections, we introduce GWCloud, a searchable repository for the creation and
curation of gravitational-wave inference products. It is designed with five
pillars in mind: uniformity of results, reproducibility of results, stability
of results, access to the astronomical community, and efficient use of
computing resources. We describe how to use GWCloud with examples, which
readers can replicate using the companion code to this paper. We describe our
long-term vision for GWCloud.

### Title: Estimating Excess COVID-19 Infections with Nonparametric Self-Exciting Point Processes
* Paper ID: 2204.13266v1
* Paper URL: [http://arxiv.org/abs/2204.13266v1](http://arxiv.org/abs/2204.13266v1)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: The COVID-19 pandemic has led to a vast amount of growth for statistical
models and methods which characterize features of disease outbreaks. One class
of models that came to light in this regard has been the use of self-exciting
point processes, wherein infections occur both "at random" and also more
systematically from person-to-person transmission. Beyond the modeling of the
overall COVID-19 outbreak, the pandemic has also motivated research assessing
various policy decisions and event outcomes. One such area of study, addressed
here, relates to the formulation of methods which measure the impact that large
events or gatherings of people had in the local areas where the events were
held. We formulate an alternative approach to traditional causal inference
methods and then apply our method to assessing the impact that then President
Donald Trump's re-election campaign rallies had on COVID-19 infections in areas
where the rallies were hosted. By incorporating several adaptions to
nonparametric self-exciting point process models, we estimate both the excess
number of COVID-19 infections brought on by the rallies and the duration of
time in which these excess infections persisted.

### Title: Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning
* Paper ID: 2204.13060v2
* Paper URL: [http://arxiv.org/abs/2204.13060v2](http://arxiv.org/abs/2204.13060v2)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: Building generalizable goal-conditioned agents from rich observations is a
key to reinforcement learning (RL) solving real world problems. Traditionally
in goal-conditioned RL, an agent is provided with the exact goal they intend to
reach. However, it is often not realistic to know the configuration of the goal
before performing a task. A more scalable framework would allow us to provide
the agent with an example of an analogous task, and have the agent then infer
what the goal should be for its current state. We propose a new form of state
abstraction called goal-conditioned bisimulation that captures functional
equivariance, allowing for the reuse of skills to achieve new goals. We learn
this representation using a metric form of this abstraction, and show its
ability to generalize to new goals in simulation manipulation tasks. Further,
we prove that this learned representation is sufficient not only for goal
conditioned tasks, but is amenable to any downstream task described by a
state-only reward function. Videos can be found at
https://sites.google.com/view/gc-bisimulation.

### Title: On the gravitational lensing interpretation of three gravitational wave detections in the mass gap by LIGO and Virgo
* Paper ID: 2204.12978v2
* Paper URL: [http://arxiv.org/abs/2204.12978v2](http://arxiv.org/abs/2204.12978v2)
* Updated Date: 2022-04-28
* Code URL: null
* Summary: We search for gravitational wave (GW) events from LIGO-Virgo's third run that
may have been affected by gravitational lensing. Gravitational lensing delays
the arrival of GWs, and alters their amplitude -- thus biasing the inferred
progenitor masses. This would provide a physically well-understood
interpretation of GW detections in the "mass gap" between neutron stars and
black holes, as gravitationally lensed binary neutron star (BNS) mergers. We
selected three GW detections in LIGO-Virgo's third run for which the
probability of at least one of the constituent compact objects being in the
mass gap was reported as high with low latency -- i.e. candidate lensed BNS
mergers. Our observations of powerful strong lensing clusters located adjacent
to the peak of their sky localisation error maps reached a sensitivity $\rm
AB\simeq25.5$ in the $z'$-band with the GMOS instruments on the Gemini
telescopes, and detected no candidate lensed optical counterparts. We combine
recent kilonova lightcurve models with recent predictions of the lensed BNS
population and the properties of the objects that we followed up to show that
realistic optical counterparts were detectable in our observations. Further
detailed analysis of two of the candidates suggests that they are a plausible
pair of images of the same low-mass binary black hole merger, lensed by a local
galaxy or small group of galaxies. This further underlines that access to
accurate mass information with low latency would improve the efficiency of
candidate lensed NS-NS selection.

### Title: Zero-Shot Logit Adjustment
* Paper ID: 2204.11822v2
* Paper URL: [http://arxiv.org/abs/2204.11822v2](http://arxiv.org/abs/2204.11822v2)
* Updated Date: 2022-04-28
* Code URL: [https://github.com/cdb342/ijcai-2022-zla](https://github.com/cdb342/ijcai-2022-zla)
* Summary: Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses
challenges in recognizing novel classes in the test phase. The development of
generative models enables current GZSL techniques to probe further into the
semantic-visual link, culminating in a two-stage form that includes a generator
and a classifier. However, existing generation-based methods focus on enhancing
the generator's effect while neglecting the improvement of the classifier. In
this paper, we first analyze of two properties of the generated pseudo unseen
samples: bias and homogeneity. Then, we perform variational Bayesian inference
to back-derive the evaluation metrics, which reflects the balance of the seen
and unseen classes. As a consequence of our derivation, the aforementioned two
properties are incorporated into the classifier training as seen-unseen priors
via logit adjustment. The Zero-Shot Logit Adjustment further puts
semantic-based classifiers into effect in generation-based GZSL. Our
experiments demonstrate that the proposed technique achieves state-of-the-art
when combined with the basic generator, and it can improve various generative
zero-shot learning frameworks. Our codes are available on
https://github.com/cdb342/IJCAI-2022-ZLA.

