### Title: TorchSparse: Efficient Point Cloud Inference Engine
* Paper ID: 2204.10319v1
* Paper URL: [http://arxiv.org/abs/2204.10319v1](http://arxiv.org/abs/2204.10319v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/mit-han-lab/torchsparse](https://github.com/mit-han-lab/torchsparse)
* Summary: Deep learning on point clouds has received increased attention thanks to its
wide applications in AR/VR and autonomous driving. These applications require
low latency and high accuracy to provide real-time user experience and ensure
user safety. Unlike conventional dense workloads, the sparse and irregular
nature of point clouds poses severe challenges to running sparse CNNs
efficiently on the general-purpose hardware. Furthermore, existing sparse
acceleration techniques for 2D images do not translate to 3D point clouds. In
this paper, we introduce TorchSparse, a high-performance point cloud inference
engine that accelerates the sparse convolution computation on GPUs. TorchSparse
directly optimizes the two bottlenecks of sparse convolution: irregular
computation and data movement. It applies adaptive matrix multiplication
grouping to trade computation for better regularity, achieving 1.4-1.5x speedup
for matrix multiplication. It also optimizes the data movement by adopting
vectorized, quantized and fused locality-aware memory access, reducing the
memory movement cost by 2.7x. Evaluated on seven representative models across
three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured
end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv,
respectively.

### Title: Feature anomaly detection system (FADS) for intelligent manufacturing
* Paper ID: 2204.10318v1
* Paper URL: [http://arxiv.org/abs/2204.10318v1](http://arxiv.org/abs/2204.10318v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Anomaly detection is important for industrial automation and part quality
assurance, and while humans can easily detect anomalies in components given a
few examples, designing a generic automated system that can perform at human or
above human capabilities remains a challenge. In this work, we present a simple
new anomaly detection algorithm called FADS (feature-based anomaly detection
system) which leverages pretrained convolutional neural networks (CNN) to
generate a statistical model of nominal inputs by observing the activation of
the convolutional filters. During inference the system compares the
convolutional filter activation of the new input to the statistical model and
flags activations that are outside the expected range of values and therefore
likely an anomaly. By using a pretrained network, FADS demonstrates excellent
performance similar to or better than other machine learning approaches to
anomaly detection while at the same time FADS requires no tuning of the CNN
weights. We demonstrate FADS ability by detecting process parameter changes on
a custom dataset of additively manufactured lattices. The FADS localization
algorithm shows that textural differences that are visible on the surface can
be used to detect process parameter changes. In addition, we test FADS on
benchmark datasets, such as the MVTec Anomaly Detection dataset, and report
good results.

### Title: SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in the Entity Space
* Paper ID: 2204.10245v1
* Paper URL: [http://arxiv.org/abs/2204.10245v1](http://arxiv.org/abs/2204.10245v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Translation distance based knowledge graph embedding (KGE) methods, such as
TransE and RotatE, model the relation in knowledge graphs as translation or
rotation in the vector space. Both translation and rotation are injective; that
is, the translation or rotation of different vectors results in different
results. In knowledge graphs, different entities may have a relation with the
same entity; for example, many actors starred in one movie. Such a
non-injective relation pattern cannot be well modeled by the translation or
rotation operations in existing translation distance based KGE methods. To
tackle the challenge, we propose a translation distance-based KGE method called
SpaceE to model relations as linear transformations. The proposed SpaceE embeds
both entities and relations in knowledge graphs as matrices and SpaceE
naturally models non-injective relations with singular linear transformations.
We theoretically demonstrate that SpaceE is a fully expressive model with the
ability to infer multiple desired relation patterns, including symmetry,
skew-symmetry, inversion, Abelian composition, and non-Abelian composition.
Experimental results on link prediction datasets illustrate that SpaceE
substantially outperforms many previous translation distance based knowledge
graph embedding methods, especially on datasets with many non-injective
relations. The code is available based on the PaddlePaddle deep learning
platform https://www.paddlepaddle.org.cn.

### Title: BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training
* Paper ID: 2204.10209v1
* Paper URL: [http://arxiv.org/abs/2204.10209v1](http://arxiv.org/abs/2204.10209v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The task of 2D human pose estimation is challenging as the number of
keypoints is typically large (~ 17) and this necessitates the use of robust
neural network architectures and training pipelines that can capture the
relevant features from the input image. These features are then aggregated to
make accurate heatmap predictions from which the final keypoints of human body
parts can be inferred. Many papers in literature use CNN-based architectures
for the backbone, and/or combine it with a transformer, after which the
features are aggregated to make the final keypoint predictions [1]. In this
paper, we consider the recently proposed Bottleneck Transformers [2], which
combine CNN and multi-head self attention (MHSA) layers effectively, and we
integrate it with a Transformer encoder and apply it to the task of 2D human
pose estimation. We consider different backbone architectures and pre-train
them using the DINO self-supervised learning method [3], this pre-training is
found to improve the overall prediction accuracy. We call our model BTranspose,
and experiments show that on the COCO validation set, our model achieves an AP
of 76.4, which is competitive with other methods such as [1] and has fewer
network parameters. Furthermore, we also present the dependencies of the final
predicted keypoints on both the MHSA block and the Transformer encoder layers,
providing clues on the image sub-regions the network attends to at the mid and
high levels.

### Title: WebFace260M: A Benchmark for Million-Scale Deep Face Recognition
* Paper ID: 2204.10149v1
* Paper URL: [http://arxiv.org/abs/2204.10149v1](http://arxiv.org/abs/2204.10149v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Face benchmarks empower the research community to train and evaluate
high-performance face recognition systems. In this paper, we contribute a new
million-scale recognition benchmark, containing uncurated 4M identities/260M
faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training
data, as well as an elaborately designed time-constrained evaluation protocol.
Firstly, we collect 4M name lists and download 260M faces from the Internet.
Then, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is
devised to purify the tremendous WebFace260M, which is efficient and scalable.
To the best of our knowledge, the cleaned WebFace42M is the largest public face
recognition training set and we expect to close the data gap between academia
and industry. Referring to practical deployments, Face Recognition Under
Inference Time conStraint (FRUITS) protocol and a new test set with rich
attributes are constructed. Besides, we gather a large-scale masked face
sub-set for biometrics assessment under COVID-19. For a comprehensive
evaluation of face matchers, three recognition tasks are performed under
standard, masked and unbiased settings, respectively. Equipped with this
benchmark, we delve into million-scale face recognition problems. A distributed
framework is developed to train face recognition models efficiently without
tampering with the performance. Enabled by WebFace42M, we reduce 40% failure
rate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT.
Even 10% data (WebFace4M) shows superior performance compared with the public
training sets. Furthermore, comprehensive baselines are established under the
FRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows
enormous potential on standard, masked and unbiased face recognition scenarios.
Our WebFace260M website is https://www.face-benchmark.org.

### Title: Toward Fast, Flexible, and Robust Low-Light Image Enhancement
* Paper ID: 2204.10137v1
* Paper URL: [http://arxiv.org/abs/2204.10137v1](http://arxiv.org/abs/2204.10137v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/vis-opt-group/sci](https://github.com/vis-opt-group/sci)
* Summary: Existing low-light image enhancement techniques are mostly not only difficult
to deal with both visual quality and computational efficiency but also commonly
invalid in unknown complex scenarios. In this paper, we develop a new
Self-Calibrated Illumination (SCI) learning framework for fast, flexible, and
robust brightening images in real-world low-light scenarios. To be specific, we
establish a cascaded illumination learning process with weight sharing to
handle this task. Considering the computational burden of the cascaded pattern,
we construct the self-calibrated module which realizes the convergence between
results of each stage, producing the gains that only use the single basic block
for inference (yet has not been exploited in previous works), which drastically
diminishes computation cost. We then define the unsupervised training loss to
elevate the model capability that can adapt to general scenes. Further, we make
comprehensive explorations to excavate SCI's inherent properties (lacking in
existing works) including operation-insensitive adaptability (acquiring stable
performance under the settings of different simple operations) and
model-irrelevant generality (can be applied to illumination-based existing
works to improve performance). Finally, plenty of experiments and ablation
studies fully indicate our superiority in both quality and efficiency.
Applications on low-light face detection and nighttime semantic segmentation
fully reveal the latent practical values for SCI. The source code is available
at https://github.com/vis-opt-group/SCI.

### Title: OSSO: Obtaining Skeletal Shape from Outside
* Paper ID: 2204.10129v1
* Paper URL: [http://arxiv.org/abs/2204.10129v1](http://arxiv.org/abs/2204.10129v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/MarilynKeller/OSSO](https://github.com/MarilynKeller/OSSO)
* Summary: We address the problem of inferring the anatomic skeleton of a person, in an
arbitrary pose, from the 3D surface of the body; i.e. we predict the inside
(bones) from the outside (skin). This has many applications in medicine and
biomechanics. Existing state-of-the-art biomechanical skeletons are detailed
but do not easily generalize to new subjects. Additionally, computer vision and
graphics methods that predict skeletons are typically heuristic, not learned
from data, do not leverage the full 3D body surface, and are not validated
against ground truth. To our knowledge, our system, called OSSO (Obtaining
Skeletal Shape from Outside), is the first to learn the mapping from the 3D
body surface to the internal skeleton from real data. We do so using 1000 male
and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit
a parametric 3D body shape model (STAR) to capture the body surface and a novel
part-based 3D skeleton model to capture the bones. This provides inside/outside
training pairs. We model the statistical variation of full skeletons using PCA
in a pose-normalized space. We then train a regressor from body shape
parameters to skeleton shape parameters and refine the skeleton to satisfy
constraints on physical plausibility. Given an arbitrary 3D body shape and
pose, OSSO predicts a realistic skeleton inside. In contrast to previous work,
we evaluate the accuracy of the skeleton shape quantitatively on held-out DXA
scans, outperforming the state-of-the-art. We also show 3D skeleton prediction
from varied and challenging 3D bodies. The code to infer a skeleton from a body
shape is available for research at https://osso.is.tue.mpg.de/, and the dataset
of paired outer surface (skin) and skeleton (bone) meshes is available as a
Biobank Returned Dataset. This research has been conducted using the UK Biobank
Resource.

### Title: Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach
* Paper ID: 2204.10090v1
* Paper URL: [http://arxiv.org/abs/2204.10090v1](http://arxiv.org/abs/2204.10090v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Collecting paired training data is difficult in practice, but the unpaired
samples broadly exist. Current approaches aim at generating synthesized
training data from the unpaired samples by exploring the relationship between
the corrupted and clean data. This work proposes LUD-VAE, a deep generative
method to learn the joint probability density function from data sampled from
marginal distributions. Our approach is based on a carefully designed
probabilistic graphical model in which the clean and corrupted data domains are
conditionally independent. Using variational inference, we maximize the
evidence lower bound (ELBO) to estimate the joint probability density function.
Furthermore, we show that the ELBO is computable without paired samples under
the inference invariant assumption. This property provides the mathematical
rationale of our approach in the unpaired setting. Finally, we apply our method
to real-world image denoising and super-resolution tasks and train the models
using the synthetic data generated by the LUD-VAE. Experimental results
validate the advantages of our method over other learnable approaches.

### Title: Time Window Frechet and Metric-Based Edit Distance for Passively Collected Trajectories
* Paper ID: 2204.10053v1
* Paper URL: [http://arxiv.org/abs/2204.10053v1](http://arxiv.org/abs/2204.10053v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The advances of modern localization techniques and the wide spread of mobile
devices have provided us great opportunities to collect and mine human mobility
trajectories. In this work, we focus on passively collected trajectories, which
are sequences of time-stamped locations that mobile entities visit. To analyse
such trajectories, a crucial part is a measure of similarity between two
trajectories. We propose the time-window Frechet distance, which enforces the
maximum temporal separation between points of two trajectories that can be
paired in the calculation of the Frechet distance, and the metric-based edit
distance which incorporates the underlying metric in the computation of the
insertion and deletion costs. Using these measures, we can cluster trajectories
to infer group motion patterns. We look at the $k$-gather problem which
requires each cluster to have at least $k$ trajectories. We prove that k-gather
remains NP-hard under edit distance, metric-based edit distance and Jaccard
distance. Finally, we improve over previous results on discrete Frechet
distance and show that there is no strongly sub-quadratic time with
approximation factor less than $1.61$ in two dimensional setting unless SETH
fails.

### Title: Understanding the Domain Gap in LiDAR Object Detection Networks
* Paper ID: 2204.10024v1
* Paper URL: [http://arxiv.org/abs/2204.10024v1](http://arxiv.org/abs/2204.10024v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: In order to make autonomous driving a reality, artificial neural networks
have to work reliably in the open-world. However, the open-world is vast and
continuously changing, so it is not technically feasible to collect and
annotate training datasets which accurately represent this domain. Therefore,
there are always domain gaps between training datasets and the open-world which
must be understood. In this work, we investigate the domain gaps between
high-resolution and low-resolution LiDAR sensors in object detection networks.
Using a unique dataset, which enables us to study sensor resolution domain gaps
independent of other effects, we show two distinct domain gaps - an inference
domain gap and a training domain gap. The inference domain gap is characterised
by a strong dependence on the number of LiDAR points per object, while the
training gap shows no such dependence. These fndings show that different
approaches are required to close these inference and training domain gaps.

### Title: Hardy spaces and quasiconformal maps in the Heisenberg group
* Paper ID: 2204.10016v1
* Paper URL: [http://arxiv.org/abs/2204.10016v1](http://arxiv.org/abs/2204.10016v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: We define Hardy spaces $H^p$, $0<p<\infty$, for quasiconformal mappings on
the Kor\'{a}nyi unit ball $B$ in the first Heisenberg group $\mathbb{H}^1$. Our
definition is stated in terms of the Heisenberg polar coordinates introduced by
Kor\'{a}nyi and Reimann, and Balogh and Tyson. First, we prove the existence of
$p_0(K)>0$ such that every $K$-quasiconformal map $f:B \to f(B) \subset
\mathbb{H}^1$ belongs to $H^p$ for all $0<p<p_0(K)$. Second, we give two
equivalent conditions for the $H^p$ membership of a quasiconformal map $f$, one
in terms of the radial limits of $f$, and one using a nontangential maximal
function of $f$. As an application, we characterize Carleson measures on $B$
via integral inequalities for quasiconformal mappings on $B$ and their radial
limits. Our paper thus extends results by Astala and Koskela, Jerison and
Weitsman, Nolder, and Zinsmeister, from $\mathbb{R}^n$ to $\mathbb{H}^1$. A
crucial difference between the proofs in $\mathbb{R}^n$ and $\mathbb{H}^1$ is
caused by the nonisotropic nature of the Kor\'{a}nyi unit sphere with its two
characteristic points.

### Title: Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach
* Paper ID: 2204.09992v1
* Paper URL: [http://arxiv.org/abs/2204.09992v1](http://arxiv.org/abs/2204.09992v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Conventional model quantization methods use a fixed quantization scheme to
different data samples, which ignores the inherent "recognition difficulty"
differences between various samples. We propose to feed different data samples
with varying quantization schemes to achieve a data-dependent dynamic
inference, at a fine-grained layer level. However, enabling this adaptive
inference with changeable layer-wise quantization schemes is challenging
because the combination of bit-widths and layers is growing exponentially,
making it extremely difficult to train a single model in such a vast searching
space and use it in practice. To solve this problem, we present the Arbitrary
Bit-width Network (ABN), where the bit-widths of a single deep network can
change at runtime for different data samples, with a layer-wise granularity.
Specifically, first we build a weight-shared layer-wise quantizable
"super-network" in which each layer can be allocated with multiple bit-widths
and thus quantized differently on demand. The super-network provides a
considerably large number of combinations of bit-widths and layers, each of
which can be used during inference without retraining or storing myriad models.
Second, based on the well-trained super-network, each layer's runtime bit-width
selection decision is modeled as a Markov Decision Process (MDP) and solved by
an adaptive inference strategy accordingly. Experiments show that the
super-network can be built without accuracy degradation, and the bit-widths
allocation of each layer can be adjusted to deal with various inputs on the
fly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement
while saving 36.2% BitOps.

### Title: CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation
* Paper ID: 2204.09914v1
* Paper URL: [http://arxiv.org/abs/2204.09914v1](http://arxiv.org/abs/2204.09914v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: LiDAR semantic segmentation essential for advanced autonomous driving is
required to be accurate, fast, and easy-deployed on mobile platforms. Previous
point-based or sparse voxel-based methods are far away from real-time
applications since time-consuming neighbor searching or sparse 3D convolution
are employed. Recent 2D projection-based methods, including range view and
multi-view fusion, can run in real time, but suffer from lower accuracy due to
information loss during the 2D projection. Besides, to improve the performance,
previous methods usually adopt test time augmentation (TTA), which further
slows down the inference process. To achieve a better speed-accuracy trade-off,
we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both
effectiveness and efficiency mainly by the following two techniques: 1) the
novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D
projected grid for efficiency, while summarizes both 2D and 3D features on 3D
point for minimal information loss; 2) the proposed transformation consistency
loss narrows the gap between the single-time model inference and TTA. The
experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the
CPGNet without ensemble models or TTA is comparable with the state-of-the-art
RPVNet, while it runs 4.7 times faster.

### Title: Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation
* Paper ID: 2204.09903v1
* Paper URL: [http://arxiv.org/abs/2204.09903v1](http://arxiv.org/abs/2204.09903v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/chunbolang/DCP](https://github.com/chunbolang/DCP)
* Summary: Few-shot segmentation, which aims to segment unseen-class objects given only
a handful of densely labeled samples, has received widespread attention from
the community. Existing approaches typically follow the prototype learning
paradigm to perform meta-inference, which fails to fully exploit the underlying
information from support image-mask pairs, resulting in various segmentation
failures, e.g., incomplete objects, ambiguous boundaries, and distractor
activation. To this end, we propose a simple yet versatile framework in the
spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is
first implemented on the annotated support image, and then the coarse
segmentation mask is divided into multiple regions with different properties.
Leveraging effective masked average pooling operations, a series of
support-induced proxies are thus derived, each playing a specific role in
conquering the above challenges. Moreover, we devise a unique parallel decoder
structure that integrates proxies with similar attributes to boost the
discrimination power. Our proposed approach, named divide-and-conquer proxies
(DCP), allows for the development of appropriate and reliable information as a
guide at the "episode" level, not just about the object cues themselves.
Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of
DCP over conventional prototype-based approaches (up to 5~10% on average),
which also establishes a new state-of-the-art. Code is available at
github.com/chunbolang/DCP.

### Title: Functional Horseshoe Smoothing for Functional Trend Estimation
* Paper ID: 2204.09898v1
* Paper URL: [http://arxiv.org/abs/2204.09898v1](http://arxiv.org/abs/2204.09898v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Due to developments in instruments and computers, functional observations are
increasingly popular. However, effective methodologies for flexibly estimating
the underlying trends with valid uncertainty quantification for a sequence of
functional data (e.g. functional time series) are still scarce. In this work,
we develop a locally adaptive smoothing method, called functional horseshoe
smoothing, by introducing a shrinkage prior to the general order of differences
of functional variables. This allows us to capture abrupt changes by taking
advantage of the shrinkage capability and also to assess uncertainty by
Bayesian inference. The fully Bayesian framework also allows the selection of
the number of basis functions via the posterior predictive loss. Also, by
taking advantage of the nature of functional data, this method is able to
handle heterogeneously observed data without data augmentation. We show the
theoretical properties of the proposed prior distribution and the posterior
mean, and finally demonstrate them through simulation studies and applications
to a real-world dataset.

### Title: Non-autoregressive Model for Full-line Code Completion
* Paper ID: 2204.09877v1
* Paper URL: [http://arxiv.org/abs/2204.09877v1](http://arxiv.org/abs/2204.09877v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Code completion tools are frequently used by software developers to
accelerate software development by suggesting the following code elements.
Completing a sequence of code tokens (e.g., a full line of code) has been
proved more efficient than predicting a single token at a time. To complete the
code sequence, researchers are employing AutoRegressive (AR) decoders to
generate tokens in a left-to-right, token-by-token fashion. Consequently, the
prediction of the next token depends on all previously generated tokens, which
leads to high latency in inference. To improve the efficiency and accuracy of
full-line code completion, in this paper, we propose a Non-AutoRegressive (NAR)
model for code completion boosted by a syntax-aware sampling strategy. Our
experimental results on two widely used datasets suggest that our model
outperforms both AR and NAR baselines on full-line code completion, and it is
faster than the AR model with up to 9 times speed-up.

### Title: Gaussian Processes for real-time 3D motion and uncertainty estimation during MR-guided radiotherapy
* Paper ID: 2204.09873v1
* Paper URL: [http://arxiv.org/abs/2204.09873v1](http://arxiv.org/abs/2204.09873v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Respiratory motion during radiotherapy causes uncertainty in the tumor's
location, which is typically addressed by an increased radiation area and a
decreased dose. As a result, the treatments' efficacy is reduced. The recently
proposed hybrid MR-linac scanner holds the promise to efficiently deal with
such respiratory motion through real-time adaptive MR-guided radiotherapy
(MRgRT). For MRgRT, motion-fields should be estimated from MR-data and the
radiotherapy plan should be adapted in real-time according to the estimated
motion-fields. All of this should be performed with a total latency of
maximally 200 ms, including data acquisition and reconstruction. A measure of
confidence in such estimated motion-fields is highly desirable, for instance to
ensure the patient's safety in case of unexpected and undesirable motion. In
this work, we propose a framework based on Gaussian Processes to infer 3D
motion-fields and uncertainty maps in real-time from only three readouts of
MR-data. We demonstrated an inference frame rate up to 69 Hz including data
acquisition and reconstruction, thereby exploiting the limited amount of
required MR-data. Additionally, we designed a rejection criterion based on the
motion-field uncertainty maps to demonstrate the framework's potential for
quality assurance. The framework was validated in silico and in vivo on healthy
volunteer data (n=5) acquired using an MR-linac, thereby taking into account
different breathing patterns and controlled bulk motion. Results indicate
end-point-errors with a 75th percentile below 1mm in silico, and a correct
detection of erroneous motion estimates with the rejection criterion.
Altogether, the results show the potential of the framework for application in
real-time MR-guided radiotherapy with an MR-linac.

### Title: The $Î¸$-augmented model for Bayesian semiparametric inference on functional parameters
* Paper ID: 2204.09862v1
* Paper URL: [http://arxiv.org/abs/2204.09862v1](http://arxiv.org/abs/2204.09862v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Semiparametric Bayesian inference has so far relied on models for the
observable that partition into two parts, one being parametric and the other
nonparametric, with the target parameter being dependent on the parametric
component. While a partitioned structure makes specification of the marginal
prior on the target parameter simple to perform, it often arises from
conditional modelling which is subject to misspecification and ultimately a
lack of consistency. We introduce a new type of semiparametric model to allow
easy prior specification for a parameter that is defined as a functional of the
distribution for the observable. Our semiparametric model is obtained as an
extension of nonparametric models that are consistent under very general
conditions. This type of Bayesian semiparametric model can be used to obtain
Bayesian versions of Frequentist estimators that are defined as functionals of
the empirical distribution. This gives us new opportunities to conduct Bayesian
analysis in problems where Frequentist estimators exist but not well-accepted
likelihoods.

### Title: Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and Local Information
* Paper ID: 2204.09860v1
* Paper URL: [http://arxiv.org/abs/2204.09860v1](http://arxiv.org/abs/2204.09860v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/xiaoyuan1996/galr](https://github.com/xiaoyuan1996/galr)
* Summary: Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become
an urgent research hotspot due to its ability of enabling fast and flexible
information extraction on remote sensing (RS) images. However, current RSCTIR
methods mainly focus on global features of RS images, which leads to the
neglect of local features that reflect target relationships and saliency. In
this article, we first propose a novel RSCTIR framework based on global and
local information (GaLR), and design a multi-level information dynamic fusion
(MIDF) module to efficaciously integrate features of different levels. MIDF
leverages local information to correct global information, utilizes global
information to supplement local information, and uses the dynamic addition of
the two to generate prominent visual representation. To alleviate the pressure
of the redundant targets on the graph convolution network (GCN) and to improve
the model s attention on salient instances during modeling local features, the
de-noised representation matrix and the enhanced adjacency matrix (DREA) are
devised to assist GCN in producing superior local representations. DREA not
only filters out redundant features with high similarity, but also obtains more
powerful local features by enhancing the features of prominent objects.
Finally, to make full use of the information in the similarity matrix during
inference, we come up with a plug-and-play multivariate rerank (MR) algorithm.
The algorithm utilizes the k nearest neighbors of the retrieval results to
perform a reverse search, and improves the performance by combining multiple
components of bidirectional retrieval. Extensive experiments on public datasets
strongly demonstrate the state-of-the-art performance of GaLR methods on the
RSCTIR task. The code of GaLR method, MR algorithm, and corresponding files
have been made available at https://github.com/xiaoyuan1996/GaLR .

### Title: A Masked Image Reconstruction Network for Document-level Relation Extraction
* Paper ID: 2204.09851v1
* Paper URL: [http://arxiv.org/abs/2204.09851v1](http://arxiv.org/abs/2204.09851v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Document-level relation extraction aims to extract relations among entities
within a document. Compared with its sentence-level counterpart, Document-level
relation extraction requires inference over multiple sentences to extract
complex relational triples. Previous research normally complete reasoning
through information propagation on the mention-level or entity-level
document-graphs, regardless of the correlations between the relationships. In
this paper, we propose a novel Document-level Relation Extraction model based
on a Masked Image Reconstruction network (DRE-MIR), which models inference as a
masked image reconstruction problem to capture the correlations between
relationships. Specifically, we first leverage an encoder module to get the
features of entities and construct the entity-pair matrix based on the
features. After that, we look on the entity-pair matrix as an image and then
randomly mask it and restore it through an inference module to capture the
correlations between the relationships. We evaluate our model on three public
document-level relation extraction datasets, i.e. DocRED, CDR, and GDA.
Experimental results demonstrate that our model achieves state-of-the-art
performance on these three datasets and has excellent robustness against the
noises during the inference process.

### Title: FedCL: Federated Contrastive Learning for Privacy-Preserving Recommendation
* Paper ID: 2204.09850v1
* Paper URL: [http://arxiv.org/abs/2204.09850v1](http://arxiv.org/abs/2204.09850v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Contrastive learning is widely used for recommendation model learning, where
selecting representative and informative negative samples is critical. Existing
methods usually focus on centralized data, where abundant and high-quality
negative samples are easy to obtain. However, centralized user data storage and
exploitation may lead to privacy risks and concerns, while decentralized user
data on a single client can be too sparse and biased for accurate contrastive
learning. In this paper, we propose a federated contrastive learning method
named FedCL for privacy-preserving recommendation, which can exploit
high-quality negative samples for effective model training with privacy well
protected. We first infer user embeddings from local user data through the
local model on each client, and then perturb them with local differential
privacy (LDP) before sending them to a central server for hard negative
sampling. Since individual user embedding contains heavy noise due to LDP, we
propose to cluster user embeddings on the server to mitigate the influence of
noise, and the cluster centroids are used to retrieve hard negative samples
from the item pool. These hard negative samples are delivered to user clients
and mixed with the observed negative samples from local data as well as
in-batch negatives constructed from positive samples for federated model
training. Extensive experiments on four benchmark datasets show FedCL can
empower various recommendation methods in a privacy-preserving way.

### Title: 6GAN: IPv6 Multi-Pattern Target Generation via Generative Adversarial Nets with Reinforcement Learning
* Paper ID: 2204.09839v1
* Paper URL: [http://arxiv.org/abs/2204.09839v1](http://arxiv.org/abs/2204.09839v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/cuitianyu961030/6gan](https://github.com/cuitianyu961030/6gan)
* Summary: Global IPv6 scanning has always been a challenge for researchers because of
the limited network speed and computational power. Target generation algorithms
are recently proposed to overcome the problem for Internet assessments by
predicting a candidate set to scan. However, IPv6 custom address configuration
emerges diverse addressing patterns discouraging algorithmic inference.
Widespread IPv6 alias could also mislead the algorithm to discover aliased
regions rather than valid host targets. In this paper, we introduce 6GAN, a
novel architecture built with Generative Adversarial Net (GAN) and
reinforcement learning for multi-pattern target generation. 6GAN forces
multiple generators to train with a multi-class discriminator and an alias
detector to generate non-aliased active targets with different addressing
pattern types. The rewards from the discriminator and the alias detector help
supervise the address sequence decision-making process. After adversarial
training, 6GAN's generators could keep a strong imitating ability for each
pattern and 6GAN's discriminator obtains outstanding pattern discrimination
ability with a 0.966 accuracy. Experiments indicate that our work outperformed
the state-of-the-art target generation algorithms by reaching a higher-quality
candidate set.

### Title: Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing
* Paper ID: 2204.09817v1
* Paper URL: [http://arxiv.org/abs/2204.09817v1](http://arxiv.org/abs/2204.09817v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.

### Title: TorchSparse: Efficient Point Cloud Inference Engine
* Paper ID: 2204.10319v1
* Paper URL: [http://arxiv.org/abs/2204.10319v1](http://arxiv.org/abs/2204.10319v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/mit-han-lab/torchsparse](https://github.com/mit-han-lab/torchsparse)
* Summary: Deep learning on point clouds has received increased attention thanks to its
wide applications in AR/VR and autonomous driving. These applications require
low latency and high accuracy to provide real-time user experience and ensure
user safety. Unlike conventional dense workloads, the sparse and irregular
nature of point clouds poses severe challenges to running sparse CNNs
efficiently on the general-purpose hardware. Furthermore, existing sparse
acceleration techniques for 2D images do not translate to 3D point clouds. In
this paper, we introduce TorchSparse, a high-performance point cloud inference
engine that accelerates the sparse convolution computation on GPUs. TorchSparse
directly optimizes the two bottlenecks of sparse convolution: irregular
computation and data movement. It applies adaptive matrix multiplication
grouping to trade computation for better regularity, achieving 1.4-1.5x speedup
for matrix multiplication. It also optimizes the data movement by adopting
vectorized, quantized and fused locality-aware memory access, reducing the
memory movement cost by 2.7x. Evaluated on seven representative models across
three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured
end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv,
respectively.

### Title: Feature anomaly detection system (FADS) for intelligent manufacturing
* Paper ID: 2204.10318v1
* Paper URL: [http://arxiv.org/abs/2204.10318v1](http://arxiv.org/abs/2204.10318v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Anomaly detection is important for industrial automation and part quality
assurance, and while humans can easily detect anomalies in components given a
few examples, designing a generic automated system that can perform at human or
above human capabilities remains a challenge. In this work, we present a simple
new anomaly detection algorithm called FADS (feature-based anomaly detection
system) which leverages pretrained convolutional neural networks (CNN) to
generate a statistical model of nominal inputs by observing the activation of
the convolutional filters. During inference the system compares the
convolutional filter activation of the new input to the statistical model and
flags activations that are outside the expected range of values and therefore
likely an anomaly. By using a pretrained network, FADS demonstrates excellent
performance similar to or better than other machine learning approaches to
anomaly detection while at the same time FADS requires no tuning of the CNN
weights. We demonstrate FADS ability by detecting process parameter changes on
a custom dataset of additively manufactured lattices. The FADS localization
algorithm shows that textural differences that are visible on the surface can
be used to detect process parameter changes. In addition, we test FADS on
benchmark datasets, such as the MVTec Anomaly Detection dataset, and report
good results.

### Title: SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in the Entity Space
* Paper ID: 2204.10245v1
* Paper URL: [http://arxiv.org/abs/2204.10245v1](http://arxiv.org/abs/2204.10245v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Translation distance based knowledge graph embedding (KGE) methods, such as
TransE and RotatE, model the relation in knowledge graphs as translation or
rotation in the vector space. Both translation and rotation are injective; that
is, the translation or rotation of different vectors results in different
results. In knowledge graphs, different entities may have a relation with the
same entity; for example, many actors starred in one movie. Such a
non-injective relation pattern cannot be well modeled by the translation or
rotation operations in existing translation distance based KGE methods. To
tackle the challenge, we propose a translation distance-based KGE method called
SpaceE to model relations as linear transformations. The proposed SpaceE embeds
both entities and relations in knowledge graphs as matrices and SpaceE
naturally models non-injective relations with singular linear transformations.
We theoretically demonstrate that SpaceE is a fully expressive model with the
ability to infer multiple desired relation patterns, including symmetry,
skew-symmetry, inversion, Abelian composition, and non-Abelian composition.
Experimental results on link prediction datasets illustrate that SpaceE
substantially outperforms many previous translation distance based knowledge
graph embedding methods, especially on datasets with many non-injective
relations. The code is available based on the PaddlePaddle deep learning
platform https://www.paddlepaddle.org.cn.

### Title: BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training
* Paper ID: 2204.10209v1
* Paper URL: [http://arxiv.org/abs/2204.10209v1](http://arxiv.org/abs/2204.10209v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The task of 2D human pose estimation is challenging as the number of
keypoints is typically large (~ 17) and this necessitates the use of robust
neural network architectures and training pipelines that can capture the
relevant features from the input image. These features are then aggregated to
make accurate heatmap predictions from which the final keypoints of human body
parts can be inferred. Many papers in literature use CNN-based architectures
for the backbone, and/or combine it with a transformer, after which the
features are aggregated to make the final keypoint predictions [1]. In this
paper, we consider the recently proposed Bottleneck Transformers [2], which
combine CNN and multi-head self attention (MHSA) layers effectively, and we
integrate it with a Transformer encoder and apply it to the task of 2D human
pose estimation. We consider different backbone architectures and pre-train
them using the DINO self-supervised learning method [3], this pre-training is
found to improve the overall prediction accuracy. We call our model BTranspose,
and experiments show that on the COCO validation set, our model achieves an AP
of 76.4, which is competitive with other methods such as [1] and has fewer
network parameters. Furthermore, we also present the dependencies of the final
predicted keypoints on both the MHSA block and the Transformer encoder layers,
providing clues on the image sub-regions the network attends to at the mid and
high levels.

### Title: WebFace260M: A Benchmark for Million-Scale Deep Face Recognition
* Paper ID: 2204.10149v1
* Paper URL: [http://arxiv.org/abs/2204.10149v1](http://arxiv.org/abs/2204.10149v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Face benchmarks empower the research community to train and evaluate
high-performance face recognition systems. In this paper, we contribute a new
million-scale recognition benchmark, containing uncurated 4M identities/260M
faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training
data, as well as an elaborately designed time-constrained evaluation protocol.
Firstly, we collect 4M name lists and download 260M faces from the Internet.
Then, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is
devised to purify the tremendous WebFace260M, which is efficient and scalable.
To the best of our knowledge, the cleaned WebFace42M is the largest public face
recognition training set and we expect to close the data gap between academia
and industry. Referring to practical deployments, Face Recognition Under
Inference Time conStraint (FRUITS) protocol and a new test set with rich
attributes are constructed. Besides, we gather a large-scale masked face
sub-set for biometrics assessment under COVID-19. For a comprehensive
evaluation of face matchers, three recognition tasks are performed under
standard, masked and unbiased settings, respectively. Equipped with this
benchmark, we delve into million-scale face recognition problems. A distributed
framework is developed to train face recognition models efficiently without
tampering with the performance. Enabled by WebFace42M, we reduce 40% failure
rate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT.
Even 10% data (WebFace4M) shows superior performance compared with the public
training sets. Furthermore, comprehensive baselines are established under the
FRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows
enormous potential on standard, masked and unbiased face recognition scenarios.
Our WebFace260M website is https://www.face-benchmark.org.

### Title: Toward Fast, Flexible, and Robust Low-Light Image Enhancement
* Paper ID: 2204.10137v1
* Paper URL: [http://arxiv.org/abs/2204.10137v1](http://arxiv.org/abs/2204.10137v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/vis-opt-group/sci](https://github.com/vis-opt-group/sci)
* Summary: Existing low-light image enhancement techniques are mostly not only difficult
to deal with both visual quality and computational efficiency but also commonly
invalid in unknown complex scenarios. In this paper, we develop a new
Self-Calibrated Illumination (SCI) learning framework for fast, flexible, and
robust brightening images in real-world low-light scenarios. To be specific, we
establish a cascaded illumination learning process with weight sharing to
handle this task. Considering the computational burden of the cascaded pattern,
we construct the self-calibrated module which realizes the convergence between
results of each stage, producing the gains that only use the single basic block
for inference (yet has not been exploited in previous works), which drastically
diminishes computation cost. We then define the unsupervised training loss to
elevate the model capability that can adapt to general scenes. Further, we make
comprehensive explorations to excavate SCI's inherent properties (lacking in
existing works) including operation-insensitive adaptability (acquiring stable
performance under the settings of different simple operations) and
model-irrelevant generality (can be applied to illumination-based existing
works to improve performance). Finally, plenty of experiments and ablation
studies fully indicate our superiority in both quality and efficiency.
Applications on low-light face detection and nighttime semantic segmentation
fully reveal the latent practical values for SCI. The source code is available
at https://github.com/vis-opt-group/SCI.

### Title: OSSO: Obtaining Skeletal Shape from Outside
* Paper ID: 2204.10129v1
* Paper URL: [http://arxiv.org/abs/2204.10129v1](http://arxiv.org/abs/2204.10129v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/MarilynKeller/OSSO](https://github.com/MarilynKeller/OSSO)
* Summary: We address the problem of inferring the anatomic skeleton of a person, in an
arbitrary pose, from the 3D surface of the body; i.e. we predict the inside
(bones) from the outside (skin). This has many applications in medicine and
biomechanics. Existing state-of-the-art biomechanical skeletons are detailed
but do not easily generalize to new subjects. Additionally, computer vision and
graphics methods that predict skeletons are typically heuristic, not learned
from data, do not leverage the full 3D body surface, and are not validated
against ground truth. To our knowledge, our system, called OSSO (Obtaining
Skeletal Shape from Outside), is the first to learn the mapping from the 3D
body surface to the internal skeleton from real data. We do so using 1000 male
and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit
a parametric 3D body shape model (STAR) to capture the body surface and a novel
part-based 3D skeleton model to capture the bones. This provides inside/outside
training pairs. We model the statistical variation of full skeletons using PCA
in a pose-normalized space. We then train a regressor from body shape
parameters to skeleton shape parameters and refine the skeleton to satisfy
constraints on physical plausibility. Given an arbitrary 3D body shape and
pose, OSSO predicts a realistic skeleton inside. In contrast to previous work,
we evaluate the accuracy of the skeleton shape quantitatively on held-out DXA
scans, outperforming the state-of-the-art. We also show 3D skeleton prediction
from varied and challenging 3D bodies. The code to infer a skeleton from a body
shape is available for research at https://osso.is.tue.mpg.de/, and the dataset
of paired outer surface (skin) and skeleton (bone) meshes is available as a
Biobank Returned Dataset. This research has been conducted using the UK Biobank
Resource.

### Title: Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach
* Paper ID: 2204.10090v1
* Paper URL: [http://arxiv.org/abs/2204.10090v1](http://arxiv.org/abs/2204.10090v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Collecting paired training data is difficult in practice, but the unpaired
samples broadly exist. Current approaches aim at generating synthesized
training data from the unpaired samples by exploring the relationship between
the corrupted and clean data. This work proposes LUD-VAE, a deep generative
method to learn the joint probability density function from data sampled from
marginal distributions. Our approach is based on a carefully designed
probabilistic graphical model in which the clean and corrupted data domains are
conditionally independent. Using variational inference, we maximize the
evidence lower bound (ELBO) to estimate the joint probability density function.
Furthermore, we show that the ELBO is computable without paired samples under
the inference invariant assumption. This property provides the mathematical
rationale of our approach in the unpaired setting. Finally, we apply our method
to real-world image denoising and super-resolution tasks and train the models
using the synthetic data generated by the LUD-VAE. Experimental results
validate the advantages of our method over other learnable approaches.

### Title: Time Window Frechet and Metric-Based Edit Distance for Passively Collected Trajectories
* Paper ID: 2204.10053v1
* Paper URL: [http://arxiv.org/abs/2204.10053v1](http://arxiv.org/abs/2204.10053v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The advances of modern localization techniques and the wide spread of mobile
devices have provided us great opportunities to collect and mine human mobility
trajectories. In this work, we focus on passively collected trajectories, which
are sequences of time-stamped locations that mobile entities visit. To analyse
such trajectories, a crucial part is a measure of similarity between two
trajectories. We propose the time-window Frechet distance, which enforces the
maximum temporal separation between points of two trajectories that can be
paired in the calculation of the Frechet distance, and the metric-based edit
distance which incorporates the underlying metric in the computation of the
insertion and deletion costs. Using these measures, we can cluster trajectories
to infer group motion patterns. We look at the $k$-gather problem which
requires each cluster to have at least $k$ trajectories. We prove that k-gather
remains NP-hard under edit distance, metric-based edit distance and Jaccard
distance. Finally, we improve over previous results on discrete Frechet
distance and show that there is no strongly sub-quadratic time with
approximation factor less than $1.61$ in two dimensional setting unless SETH
fails.

### Title: Understanding the Domain Gap in LiDAR Object Detection Networks
* Paper ID: 2204.10024v1
* Paper URL: [http://arxiv.org/abs/2204.10024v1](http://arxiv.org/abs/2204.10024v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: In order to make autonomous driving a reality, artificial neural networks
have to work reliably in the open-world. However, the open-world is vast and
continuously changing, so it is not technically feasible to collect and
annotate training datasets which accurately represent this domain. Therefore,
there are always domain gaps between training datasets and the open-world which
must be understood. In this work, we investigate the domain gaps between
high-resolution and low-resolution LiDAR sensors in object detection networks.
Using a unique dataset, which enables us to study sensor resolution domain gaps
independent of other effects, we show two distinct domain gaps - an inference
domain gap and a training domain gap. The inference domain gap is characterised
by a strong dependence on the number of LiDAR points per object, while the
training gap shows no such dependence. These fndings show that different
approaches are required to close these inference and training domain gaps.

### Title: Hardy spaces and quasiconformal maps in the Heisenberg group
* Paper ID: 2204.10016v1
* Paper URL: [http://arxiv.org/abs/2204.10016v1](http://arxiv.org/abs/2204.10016v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: We define Hardy spaces $H^p$, $0<p<\infty$, for quasiconformal mappings on
the Kor\'{a}nyi unit ball $B$ in the first Heisenberg group $\mathbb{H}^1$. Our
definition is stated in terms of the Heisenberg polar coordinates introduced by
Kor\'{a}nyi and Reimann, and Balogh and Tyson. First, we prove the existence of
$p_0(K)>0$ such that every $K$-quasiconformal map $f:B \to f(B) \subset
\mathbb{H}^1$ belongs to $H^p$ for all $0<p<p_0(K)$. Second, we give two
equivalent conditions for the $H^p$ membership of a quasiconformal map $f$, one
in terms of the radial limits of $f$, and one using a nontangential maximal
function of $f$. As an application, we characterize Carleson measures on $B$
via integral inequalities for quasiconformal mappings on $B$ and their radial
limits. Our paper thus extends results by Astala and Koskela, Jerison and
Weitsman, Nolder, and Zinsmeister, from $\mathbb{R}^n$ to $\mathbb{H}^1$. A
crucial difference between the proofs in $\mathbb{R}^n$ and $\mathbb{H}^1$ is
caused by the nonisotropic nature of the Kor\'{a}nyi unit sphere with its two
characteristic points.

### Title: Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach
* Paper ID: 2204.09992v1
* Paper URL: [http://arxiv.org/abs/2204.09992v1](http://arxiv.org/abs/2204.09992v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Conventional model quantization methods use a fixed quantization scheme to
different data samples, which ignores the inherent "recognition difficulty"
differences between various samples. We propose to feed different data samples
with varying quantization schemes to achieve a data-dependent dynamic
inference, at a fine-grained layer level. However, enabling this adaptive
inference with changeable layer-wise quantization schemes is challenging
because the combination of bit-widths and layers is growing exponentially,
making it extremely difficult to train a single model in such a vast searching
space and use it in practice. To solve this problem, we present the Arbitrary
Bit-width Network (ABN), where the bit-widths of a single deep network can
change at runtime for different data samples, with a layer-wise granularity.
Specifically, first we build a weight-shared layer-wise quantizable
"super-network" in which each layer can be allocated with multiple bit-widths
and thus quantized differently on demand. The super-network provides a
considerably large number of combinations of bit-widths and layers, each of
which can be used during inference without retraining or storing myriad models.
Second, based on the well-trained super-network, each layer's runtime bit-width
selection decision is modeled as a Markov Decision Process (MDP) and solved by
an adaptive inference strategy accordingly. Experiments show that the
super-network can be built without accuracy degradation, and the bit-widths
allocation of each layer can be adjusted to deal with various inputs on the
fly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement
while saving 36.2% BitOps.

### Title: CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation
* Paper ID: 2204.09914v1
* Paper URL: [http://arxiv.org/abs/2204.09914v1](http://arxiv.org/abs/2204.09914v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: LiDAR semantic segmentation essential for advanced autonomous driving is
required to be accurate, fast, and easy-deployed on mobile platforms. Previous
point-based or sparse voxel-based methods are far away from real-time
applications since time-consuming neighbor searching or sparse 3D convolution
are employed. Recent 2D projection-based methods, including range view and
multi-view fusion, can run in real time, but suffer from lower accuracy due to
information loss during the 2D projection. Besides, to improve the performance,
previous methods usually adopt test time augmentation (TTA), which further
slows down the inference process. To achieve a better speed-accuracy trade-off,
we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both
effectiveness and efficiency mainly by the following two techniques: 1) the
novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D
projected grid for efficiency, while summarizes both 2D and 3D features on 3D
point for minimal information loss; 2) the proposed transformation consistency
loss narrows the gap between the single-time model inference and TTA. The
experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the
CPGNet without ensemble models or TTA is comparable with the state-of-the-art
RPVNet, while it runs 4.7 times faster.

### Title: Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation
* Paper ID: 2204.09903v1
* Paper URL: [http://arxiv.org/abs/2204.09903v1](http://arxiv.org/abs/2204.09903v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/chunbolang/DCP](https://github.com/chunbolang/DCP)
* Summary: Few-shot segmentation, which aims to segment unseen-class objects given only
a handful of densely labeled samples, has received widespread attention from
the community. Existing approaches typically follow the prototype learning
paradigm to perform meta-inference, which fails to fully exploit the underlying
information from support image-mask pairs, resulting in various segmentation
failures, e.g., incomplete objects, ambiguous boundaries, and distractor
activation. To this end, we propose a simple yet versatile framework in the
spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is
first implemented on the annotated support image, and then the coarse
segmentation mask is divided into multiple regions with different properties.
Leveraging effective masked average pooling operations, a series of
support-induced proxies are thus derived, each playing a specific role in
conquering the above challenges. Moreover, we devise a unique parallel decoder
structure that integrates proxies with similar attributes to boost the
discrimination power. Our proposed approach, named divide-and-conquer proxies
(DCP), allows for the development of appropriate and reliable information as a
guide at the "episode" level, not just about the object cues themselves.
Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of
DCP over conventional prototype-based approaches (up to 5~10% on average),
which also establishes a new state-of-the-art. Code is available at
github.com/chunbolang/DCP.

### Title: Functional Horseshoe Smoothing for Functional Trend Estimation
* Paper ID: 2204.09898v1
* Paper URL: [http://arxiv.org/abs/2204.09898v1](http://arxiv.org/abs/2204.09898v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Due to developments in instruments and computers, functional observations are
increasingly popular. However, effective methodologies for flexibly estimating
the underlying trends with valid uncertainty quantification for a sequence of
functional data (e.g. functional time series) are still scarce. In this work,
we develop a locally adaptive smoothing method, called functional horseshoe
smoothing, by introducing a shrinkage prior to the general order of differences
of functional variables. This allows us to capture abrupt changes by taking
advantage of the shrinkage capability and also to assess uncertainty by
Bayesian inference. The fully Bayesian framework also allows the selection of
the number of basis functions via the posterior predictive loss. Also, by
taking advantage of the nature of functional data, this method is able to
handle heterogeneously observed data without data augmentation. We show the
theoretical properties of the proposed prior distribution and the posterior
mean, and finally demonstrate them through simulation studies and applications
to a real-world dataset.

### Title: Non-autoregressive Model for Full-line Code Completion
* Paper ID: 2204.09877v1
* Paper URL: [http://arxiv.org/abs/2204.09877v1](http://arxiv.org/abs/2204.09877v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Code completion tools are frequently used by software developers to
accelerate software development by suggesting the following code elements.
Completing a sequence of code tokens (e.g., a full line of code) has been
proved more efficient than predicting a single token at a time. To complete the
code sequence, researchers are employing AutoRegressive (AR) decoders to
generate tokens in a left-to-right, token-by-token fashion. Consequently, the
prediction of the next token depends on all previously generated tokens, which
leads to high latency in inference. To improve the efficiency and accuracy of
full-line code completion, in this paper, we propose a Non-AutoRegressive (NAR)
model for code completion boosted by a syntax-aware sampling strategy. Our
experimental results on two widely used datasets suggest that our model
outperforms both AR and NAR baselines on full-line code completion, and it is
faster than the AR model with up to 9 times speed-up.

### Title: Gaussian Processes for real-time 3D motion and uncertainty estimation during MR-guided radiotherapy
* Paper ID: 2204.09873v1
* Paper URL: [http://arxiv.org/abs/2204.09873v1](http://arxiv.org/abs/2204.09873v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Respiratory motion during radiotherapy causes uncertainty in the tumor's
location, which is typically addressed by an increased radiation area and a
decreased dose. As a result, the treatments' efficacy is reduced. The recently
proposed hybrid MR-linac scanner holds the promise to efficiently deal with
such respiratory motion through real-time adaptive MR-guided radiotherapy
(MRgRT). For MRgRT, motion-fields should be estimated from MR-data and the
radiotherapy plan should be adapted in real-time according to the estimated
motion-fields. All of this should be performed with a total latency of
maximally 200 ms, including data acquisition and reconstruction. A measure of
confidence in such estimated motion-fields is highly desirable, for instance to
ensure the patient's safety in case of unexpected and undesirable motion. In
this work, we propose a framework based on Gaussian Processes to infer 3D
motion-fields and uncertainty maps in real-time from only three readouts of
MR-data. We demonstrated an inference frame rate up to 69 Hz including data
acquisition and reconstruction, thereby exploiting the limited amount of
required MR-data. Additionally, we designed a rejection criterion based on the
motion-field uncertainty maps to demonstrate the framework's potential for
quality assurance. The framework was validated in silico and in vivo on healthy
volunteer data (n=5) acquired using an MR-linac, thereby taking into account
different breathing patterns and controlled bulk motion. Results indicate
end-point-errors with a 75th percentile below 1mm in silico, and a correct
detection of erroneous motion estimates with the rejection criterion.
Altogether, the results show the potential of the framework for application in
real-time MR-guided radiotherapy with an MR-linac.

### Title: The $Î¸$-augmented model for Bayesian semiparametric inference on functional parameters
* Paper ID: 2204.09862v1
* Paper URL: [http://arxiv.org/abs/2204.09862v1](http://arxiv.org/abs/2204.09862v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Semiparametric Bayesian inference has so far relied on models for the
observable that partition into two parts, one being parametric and the other
nonparametric, with the target parameter being dependent on the parametric
component. While a partitioned structure makes specification of the marginal
prior on the target parameter simple to perform, it often arises from
conditional modelling which is subject to misspecification and ultimately a
lack of consistency. We introduce a new type of semiparametric model to allow
easy prior specification for a parameter that is defined as a functional of the
distribution for the observable. Our semiparametric model is obtained as an
extension of nonparametric models that are consistent under very general
conditions. This type of Bayesian semiparametric model can be used to obtain
Bayesian versions of Frequentist estimators that are defined as functionals of
the empirical distribution. This gives us new opportunities to conduct Bayesian
analysis in problems where Frequentist estimators exist but not well-accepted
likelihoods.

### Title: Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and Local Information
* Paper ID: 2204.09860v1
* Paper URL: [http://arxiv.org/abs/2204.09860v1](http://arxiv.org/abs/2204.09860v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/xiaoyuan1996/galr](https://github.com/xiaoyuan1996/galr)
* Summary: Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become
an urgent research hotspot due to its ability of enabling fast and flexible
information extraction on remote sensing (RS) images. However, current RSCTIR
methods mainly focus on global features of RS images, which leads to the
neglect of local features that reflect target relationships and saliency. In
this article, we first propose a novel RSCTIR framework based on global and
local information (GaLR), and design a multi-level information dynamic fusion
(MIDF) module to efficaciously integrate features of different levels. MIDF
leverages local information to correct global information, utilizes global
information to supplement local information, and uses the dynamic addition of
the two to generate prominent visual representation. To alleviate the pressure
of the redundant targets on the graph convolution network (GCN) and to improve
the model s attention on salient instances during modeling local features, the
de-noised representation matrix and the enhanced adjacency matrix (DREA) are
devised to assist GCN in producing superior local representations. DREA not
only filters out redundant features with high similarity, but also obtains more
powerful local features by enhancing the features of prominent objects.
Finally, to make full use of the information in the similarity matrix during
inference, we come up with a plug-and-play multivariate rerank (MR) algorithm.
The algorithm utilizes the k nearest neighbors of the retrieval results to
perform a reverse search, and improves the performance by combining multiple
components of bidirectional retrieval. Extensive experiments on public datasets
strongly demonstrate the state-of-the-art performance of GaLR methods on the
RSCTIR task. The code of GaLR method, MR algorithm, and corresponding files
have been made available at https://github.com/xiaoyuan1996/GaLR .

### Title: A Masked Image Reconstruction Network for Document-level Relation Extraction
* Paper ID: 2204.09851v1
* Paper URL: [http://arxiv.org/abs/2204.09851v1](http://arxiv.org/abs/2204.09851v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Document-level relation extraction aims to extract relations among entities
within a document. Compared with its sentence-level counterpart, Document-level
relation extraction requires inference over multiple sentences to extract
complex relational triples. Previous research normally complete reasoning
through information propagation on the mention-level or entity-level
document-graphs, regardless of the correlations between the relationships. In
this paper, we propose a novel Document-level Relation Extraction model based
on a Masked Image Reconstruction network (DRE-MIR), which models inference as a
masked image reconstruction problem to capture the correlations between
relationships. Specifically, we first leverage an encoder module to get the
features of entities and construct the entity-pair matrix based on the
features. After that, we look on the entity-pair matrix as an image and then
randomly mask it and restore it through an inference module to capture the
correlations between the relationships. We evaluate our model on three public
document-level relation extraction datasets, i.e. DocRED, CDR, and GDA.
Experimental results demonstrate that our model achieves state-of-the-art
performance on these three datasets and has excellent robustness against the
noises during the inference process.

### Title: FedCL: Federated Contrastive Learning for Privacy-Preserving Recommendation
* Paper ID: 2204.09850v1
* Paper URL: [http://arxiv.org/abs/2204.09850v1](http://arxiv.org/abs/2204.09850v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Contrastive learning is widely used for recommendation model learning, where
selecting representative and informative negative samples is critical. Existing
methods usually focus on centralized data, where abundant and high-quality
negative samples are easy to obtain. However, centralized user data storage and
exploitation may lead to privacy risks and concerns, while decentralized user
data on a single client can be too sparse and biased for accurate contrastive
learning. In this paper, we propose a federated contrastive learning method
named FedCL for privacy-preserving recommendation, which can exploit
high-quality negative samples for effective model training with privacy well
protected. We first infer user embeddings from local user data through the
local model on each client, and then perturb them with local differential
privacy (LDP) before sending them to a central server for hard negative
sampling. Since individual user embedding contains heavy noise due to LDP, we
propose to cluster user embeddings on the server to mitigate the influence of
noise, and the cluster centroids are used to retrieve hard negative samples
from the item pool. These hard negative samples are delivered to user clients
and mixed with the observed negative samples from local data as well as
in-batch negatives constructed from positive samples for federated model
training. Extensive experiments on four benchmark datasets show FedCL can
empower various recommendation methods in a privacy-preserving way.

### Title: 6GAN: IPv6 Multi-Pattern Target Generation via Generative Adversarial Nets with Reinforcement Learning
* Paper ID: 2204.09839v1
* Paper URL: [http://arxiv.org/abs/2204.09839v1](http://arxiv.org/abs/2204.09839v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/cuitianyu961030/6gan](https://github.com/cuitianyu961030/6gan)
* Summary: Global IPv6 scanning has always been a challenge for researchers because of
the limited network speed and computational power. Target generation algorithms
are recently proposed to overcome the problem for Internet assessments by
predicting a candidate set to scan. However, IPv6 custom address configuration
emerges diverse addressing patterns discouraging algorithmic inference.
Widespread IPv6 alias could also mislead the algorithm to discover aliased
regions rather than valid host targets. In this paper, we introduce 6GAN, a
novel architecture built with Generative Adversarial Net (GAN) and
reinforcement learning for multi-pattern target generation. 6GAN forces
multiple generators to train with a multi-class discriminator and an alias
detector to generate non-aliased active targets with different addressing
pattern types. The rewards from the discriminator and the alias detector help
supervise the address sequence decision-making process. After adversarial
training, 6GAN's generators could keep a strong imitating ability for each
pattern and 6GAN's discriminator obtains outstanding pattern discrimination
ability with a 0.966 accuracy. Experiments indicate that our work outperformed
the state-of-the-art target generation algorithms by reaching a higher-quality
candidate set.

### Title: Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing
* Paper ID: 2204.09817v1
* Paper URL: [http://arxiv.org/abs/2204.09817v1](http://arxiv.org/abs/2204.09817v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.

### Title: Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction
* Paper ID: 2204.10436v1
* Paper URL: [http://arxiv.org/abs/2204.10436v1](http://arxiv.org/abs/2204.10436v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/ad12/meddlr](https://github.com/ad12/meddlr)
* Summary: Unrolled neural networks have enabled state-of-the-art reconstruction
performance and fast inference times for the accelerated magnetic resonance
imaging (MRI) reconstruction task. However, these approaches depend on
fully-sampled scans as ground truth data which is either costly or not possible
to acquire in many clinical medical imaging applications; hence, reducing
dependence on data is desirable. In this work, we propose modeling the proximal
operators of unrolled neural networks with scale-equivariant convolutional
neural networks in order to improve the data-efficiency and robustness to
drifts in scale of the images that might stem from the variability of patient
anatomies or change in field-of-view across different MRI scanners. Our
approach demonstrates strong improvements over the state-of-the-art unrolled
neural networks under the same memory constraints both with and without data
augmentations on both in-distribution and out-of-distribution scaled images
without significantly increasing the train or inference time.

### Title: Curriculum Learning for Goal-Oriented Semantic Communications with a Common Language
* Paper ID: 2204.10429v1
* Paper URL: [http://arxiv.org/abs/2204.10429v1](http://arxiv.org/abs/2204.10429v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Goal-oriented semantic communication will be a pillar of next-generation
wireless networks. Despite significant recent efforts in this area, most prior
works are focused on specific data types (e.g., image or audio), and they
ignore the goal and effectiveness aspects of semantic transmissions. In
contrast, in this paper, a holistic goal-oriented semantic communication
framework is proposed to enable a speaker and a listener to cooperatively
execute a set of sequential tasks in a dynamic environment. A common language
based on a hierarchical belief set is proposed to enable semantic
communications between speaker and listener. The speaker, acting as an observer
of the environment, utilizes the beliefs to transmit an initial description of
its observation (called event) to the listener. The listener is then able to
infer on the transmitted description and complete it by adding related beliefs
to the transmitted beliefs of the speaker. As such, the listener reconstructs
the observed event based on the completed description, and it then takes
appropriate action in the environment based on the reconstructed event. An
optimization problem is defined to determine the perfect and abstract
description of the events while minimizing the transmission and inference costs
with constraints on the task execution time and belief efficiency. Then, a
novel bottom-up curriculum learning (CL) framework based on reinforcement
learning is proposed to solve the optimization problem and enable the speaker
and listener to gradually identify the structure of the belief set and the
perfect and abstract description of the events. Simulation results show that
the proposed CL method outperforms traditional RL in terms of convergence time,
task execution cost and time, reliability, and belief efficiency.

### Title: Marginal Structural Illness-Death Models for Semi-Competing Risks Data
* Paper ID: 2204.10426v1
* Paper URL: [http://arxiv.org/abs/2204.10426v1](http://arxiv.org/abs/2204.10426v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The three-state illness death model has been established as a general
approach for regression analysis of semi-competing risks data. In this paper,
we apply it to a class of marginal structural models for observational data. We
consider two specific such models, the usual Markov illness-death structural
model and the general Markov illness-death structural model which incorporates
a frailty term. For interpretation purposes, risk contrasts under the
structural models are defined. Inference under the usual Markov model can be
carried out using estimating equations with inverse probability weighting,
while inference under the general Markov model requires a weighted EM
algorithm. We study the inference procedures under both models using extensive
simulations and apply them to the analysis of mid-life alcohol exposure on
late-life cognitive impairment as well as mortality using the Honolulu Asia
Aging Study data set. The R codes developed in this work have been implemented
in the R package semicmprskcoxmsm that is publicly available on CRAN.

### Title: Testing Velocity Field Lensing on Realistic Galaxy Models
* Paper ID: 2204.10415v1
* Paper URL: [http://arxiv.org/abs/2204.10415v1](http://arxiv.org/abs/2204.10415v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Weak gravitational lensing shear could be measured far more precisely if
information about unlensed attributes of source galaxies were available. Disk
galaxy velocity fields supply such information, at least in principle, with
idealized models predicting orders of magnitude more Fisher information when
velocity field observations are used to complement images. To test the level at
which realistic features of disk galaxies (warps, bars, spiral arms, and other
substructure) inject noise or bias into such shear measurements, we fit an
idealized disk model, including shear, to unsheared galaxies in the Illustris
TNG simulation. The inferred shear thus indicates the extent to which unmodeled
galaxy features inject noise and bias. We find that $\gamma_+$, the component
of shear parallel to the galaxy's first principal axis, is highly biased and
noisy because disks display a range of intrinsic axis ratios from 0.8-1. The
other shear component, $\gamma_\times$, shows little bias and is well-described
by a double Gaussian distribution with central core scatter
$\sigma_{\text{core}} \approx$ 0.03, with low-amplitude, broad wings. This is
the first measurement of the natural noise floor in the proposed velocity field
lensing technique. We conclude that the technique will achieve impressive
precision gains for measurements of $\gamma_\times$, but little gain for
measurements of $\gamma_+$.

### Title: Interpolation of Missing Swaption Volatility Data using Gibbs Sampling on Variational Autoencoders
* Paper ID: 2204.10400v1
* Paper URL: [http://arxiv.org/abs/2204.10400v1](http://arxiv.org/abs/2204.10400v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Albeit of crucial interest for both financial practitioners and researchers,
market-implied volatility data of European swaptions often exhibit large
portions of missing quotes due to illiquidity of the various underlying
swaption instruments. In this case, standard stochastic interpolation tools
like the common SABR model often cannot be calibrated to observed implied
volatility smiles, due to data being only available for the at-the-money quote
of the respective underlying swaption. Here, we propose to infer the geometry
of the full unknown implied volatility cube by learning stochastic latent
representations of implied volatility cubes via variational autoencoders,
enabling inference about the missing volatility data conditional on the
observed data by an approximate Gibbs sampling approach. Imputed estimates of
missing quotes can afterwards be used to fit a standard stochastic volatility
model. Since training data for the employed variational autoencoder model is
usually sparsely available, we test the robustness of the approach for a model
trained on synthetic data on real market quotes and we show that SABR
interpolated volatilites calibrated to reconstructed volatility cubes with
artificially imputed missing values differ by not much more than two basis
points compared to SABR fits calibrated to the complete cube. Moreover, we show
how the imputation can be used to successfully set up delta-neutral portfolios
for hedging purposes.

### Title: lpcde: Local Polynomial Conditional Density Estimation and Inference
* Paper ID: 2204.10375v1
* Paper URL: [http://arxiv.org/abs/2204.10375v1](http://arxiv.org/abs/2204.10375v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: This paper discusses the R package lpcde, which stands for local polynomial
conditional density estimation. It implements the kernel-based local polynomial
smoothing methods introduced in Cattaneo, Chandak, Jansson, Ma (2022) for
statistical estimation and inference of conditional distributions, densities,
and derivatives thereof. The package offers pointwise and integrated mean
square error optimal bandwidth selection and associated point estimators, as
well as uncertainty quantification based on robust bias correction both
pointwise (e.g., confidence intervals) and uniformly (e.g., confidence bands)
over evaluation points. The methods implemented are boundary adaptive whenever
the data is compactly supported. We contrast the functionalities of lpcde with
existing R packages, and showcase its main features using simulated data.

### Title: Boundary Adaptive Local Polynomial Conditional Density Estimators
* Paper ID: 2204.10359v1
* Paper URL: [http://arxiv.org/abs/2204.10359v1](http://arxiv.org/abs/2204.10359v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: We begin by introducing a class of conditional density estimators based on
local polynomial techniques. The estimators are automatically boundary adaptive
and easy to implement. We then study the (pointwise and) uniform statistical
properties of the estimators, offering nonasymptotic characterizations of both
probability concentration and distributional approximation. In particular, we
establish optimal uniform convergence rate in probability and valid Gaussian
distributional approximations for the t-statistic process indexed over the data
support. We also discuss implementation issues such as consistent estimation of
the covariance function of the Gaussian approximation, optimal integrated mean
squared error bandwidth selection, and valid robust bias-corrected inference.
We illustrate the applicability of our results by constructing valid confidence
bands and hypothesis tests for both parametric specification and shape
constraints, explicitly characterizing their nonasymptotic approximation
probability errors. A companion R software package implementing our main
results is provided.

### Title: Interactive Segmentation and Visualization for Tiny Objects in Multi-megapixel Images
* Paper ID: 2204.10356v1
* Paper URL: [http://arxiv.org/abs/2204.10356v1](http://arxiv.org/abs/2204.10356v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/cy-xu/cosmic-conn](https://github.com/cy-xu/cosmic-conn)
* Summary: We introduce an interactive image segmentation and visualization framework
for identifying, inspecting, and editing tiny objects (just a few pixels wide)
in large multi-megapixel high-dynamic-range (HDR) images. Detecting cosmic rays
(CRs) in astronomical observations is a cumbersome workflow that requires
multiple tools, so we developed an interactive toolkit that unifies model
inference, HDR image visualization, segmentation mask inspection and editing
into a single graphical user interface. The feature set, initially designed for
astronomical data, makes this work a useful research-supporting tool for
human-in-the-loop tiny-object segmentation in scientific areas like
biomedicine, materials science, remote sensing, etc., as well as computer
vision. Our interface features mouse-controlled, synchronized, dual-window
visualization of the image and the segmentation mask, a critical feature for
locating tiny objects in multi-megapixel images. The browser-based tool can be
readily hosted on the web to provide multi-user access and GPU acceleration for
any device. The toolkit can also be used as a high-precision annotation tool,
or adapted as the frontend for an interactive machine learning framework. Our
open-source dataset, CR detection model, and visualization toolkit are
available at https://github.com/cy-xu/cosmic-conn.

### Title: Three-point intrinsic alignments of dark matter halos in the IllustrisTNG simulation
* Paper ID: 2204.10342v1
* Paper URL: [http://arxiv.org/abs/2204.10342v1](http://arxiv.org/abs/2204.10342v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: We use the IllustrisTNG suite of cosmological simulations to measure
intrinsic alignment (IA) bispectra of dark matter subhalos between redshifts 0
and 1. We decompose the intrinsic shear field into E- and B-modes and find that
the bispectra $B_{\delta\delta\mathrm{E}}$ and $B_{\delta\mathrm{EE}}$, between
the matter overdensity field, $\delta$, and the E-mode field, are detected with
high significance. We also model the IA bispectra analytically using a method
consistent with the two-point non-linear alignment model. We use this model and
the simulation measurements to infer the intrinsic alignment amplitude
$A_\mathrm{IA}$ and find that values of $A_\mathrm{IA}$ obtained from IA power
spectra and bispectra agree well at scales up to $k_\mathrm{max}= 2 \, h
\mathrm{Mpc}^{-1}$, for example at $z=1$ $A_\mathrm{IA} = 2.13 \pm$ 0.02 from
the cross power spectrum between the matter overdensity and E-mode fields and
$A_\mathrm{IA} =2.11 \pm$ 0.03 from $B_{\delta\delta \mathrm{E}}$. This
demonstrates that a single physically motivated model can jointly model
two-point and three-point statistics of intrinsic alignments, thus enabling a
cleaner separation between intrinsic alignments and cosmological weak lensing
signals.

### Title: Revealing the Galaxy-Halo Connection Through Machine Learning
* Paper ID: 2204.10332v1
* Paper URL: [http://arxiv.org/abs/2204.10332v1](http://arxiv.org/abs/2204.10332v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Understanding the connections between galaxy stellar mass, star formation
rate, and dark matter halo mass represents a key goal of the theory of galaxy
formation. Cosmological simulations that include hydrodynamics, physical
treatments of star formation, feedback from supernovae, and the radiative
transfer of ionizing photons can capture the processes relevant for
establishing these connections. The complexity of these physics can prove
difficult to disentangle and obfuscate how mass-dependent trends in the galaxy
population originate. Here, we train a machine learning method called
Explainable Boosting Machines (EBMs) to infer how the stellar mass and star
formation rate of nearly 6 million galaxies simulated by the Cosmic
Reionization on Computers (CROC) project depend on the physical properties of
halo mass, the peak circular velocity of the galaxy during its formation
history $v_\mathrm{peak}$, cosmic environment, and redshift. The resulting EBM
models reveal the relative importance of these properties in setting galaxy
stellar mass and star formation rate, with $v_\mathrm{peak}$ providing the most
dominant contribution. Environmental properties provide substantial
improvements for modeling the stellar mass and star formation rate in only
$\lesssim10\%$ of the simulated galaxies. We also provide alternative
formulations of EBM models that enable low-resolution simulations, which cannot
track the interior structure of dark matter halos, to predict the stellar mass
and star formation rate of galaxies computed by high-resolution simulations
with detailed baryonic physics.

### Title: TorchSparse: Efficient Point Cloud Inference Engine
* Paper ID: 2204.10319v1
* Paper URL: [http://arxiv.org/abs/2204.10319v1](http://arxiv.org/abs/2204.10319v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/mit-han-lab/torchsparse](https://github.com/mit-han-lab/torchsparse)
* Summary: Deep learning on point clouds has received increased attention thanks to its
wide applications in AR/VR and autonomous driving. These applications require
low latency and high accuracy to provide real-time user experience and ensure
user safety. Unlike conventional dense workloads, the sparse and irregular
nature of point clouds poses severe challenges to running sparse CNNs
efficiently on the general-purpose hardware. Furthermore, existing sparse
acceleration techniques for 2D images do not translate to 3D point clouds. In
this paper, we introduce TorchSparse, a high-performance point cloud inference
engine that accelerates the sparse convolution computation on GPUs. TorchSparse
directly optimizes the two bottlenecks of sparse convolution: irregular
computation and data movement. It applies adaptive matrix multiplication
grouping to trade computation for better regularity, achieving 1.4-1.5x speedup
for matrix multiplication. It also optimizes the data movement by adopting
vectorized, quantized and fused locality-aware memory access, reducing the
memory movement cost by 2.7x. Evaluated on seven representative models across
three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured
end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv,
respectively.

### Title: Feature anomaly detection system (FADS) for intelligent manufacturing
* Paper ID: 2204.10318v1
* Paper URL: [http://arxiv.org/abs/2204.10318v1](http://arxiv.org/abs/2204.10318v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Anomaly detection is important for industrial automation and part quality
assurance, and while humans can easily detect anomalies in components given a
few examples, designing a generic automated system that can perform at human or
above human capabilities remains a challenge. In this work, we present a simple
new anomaly detection algorithm called FADS (feature-based anomaly detection
system) which leverages pretrained convolutional neural networks (CNN) to
generate a statistical model of nominal inputs by observing the activation of
the convolutional filters. During inference the system compares the
convolutional filter activation of the new input to the statistical model and
flags activations that are outside the expected range of values and therefore
likely an anomaly. By using a pretrained network, FADS demonstrates excellent
performance similar to or better than other machine learning approaches to
anomaly detection while at the same time FADS requires no tuning of the CNN
weights. We demonstrate FADS ability by detecting process parameter changes on
a custom dataset of additively manufactured lattices. The FADS localization
algorithm shows that textural differences that are visible on the surface can
be used to detect process parameter changes. In addition, we test FADS on
benchmark datasets, such as the MVTec Anomaly Detection dataset, and report
good results.

### Title: SpaceE: Knowledge Graph Embedding by Relational Linear Transformation in the Entity Space
* Paper ID: 2204.10245v1
* Paper URL: [http://arxiv.org/abs/2204.10245v1](http://arxiv.org/abs/2204.10245v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Translation distance based knowledge graph embedding (KGE) methods, such as
TransE and RotatE, model the relation in knowledge graphs as translation or
rotation in the vector space. Both translation and rotation are injective; that
is, the translation or rotation of different vectors results in different
results. In knowledge graphs, different entities may have a relation with the
same entity; for example, many actors starred in one movie. Such a
non-injective relation pattern cannot be well modeled by the translation or
rotation operations in existing translation distance based KGE methods. To
tackle the challenge, we propose a translation distance-based KGE method called
SpaceE to model relations as linear transformations. The proposed SpaceE embeds
both entities and relations in knowledge graphs as matrices and SpaceE
naturally models non-injective relations with singular linear transformations.
We theoretically demonstrate that SpaceE is a fully expressive model with the
ability to infer multiple desired relation patterns, including symmetry,
skew-symmetry, inversion, Abelian composition, and non-Abelian composition.
Experimental results on link prediction datasets illustrate that SpaceE
substantially outperforms many previous translation distance based knowledge
graph embedding methods, especially on datasets with many non-injective
relations. The code is available based on the PaddlePaddle deep learning
platform https://www.paddlepaddle.org.cn.

### Title: BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training
* Paper ID: 2204.10209v1
* Paper URL: [http://arxiv.org/abs/2204.10209v1](http://arxiv.org/abs/2204.10209v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The task of 2D human pose estimation is challenging as the number of
keypoints is typically large (~ 17) and this necessitates the use of robust
neural network architectures and training pipelines that can capture the
relevant features from the input image. These features are then aggregated to
make accurate heatmap predictions from which the final keypoints of human body
parts can be inferred. Many papers in literature use CNN-based architectures
for the backbone, and/or combine it with a transformer, after which the
features are aggregated to make the final keypoint predictions [1]. In this
paper, we consider the recently proposed Bottleneck Transformers [2], which
combine CNN and multi-head self attention (MHSA) layers effectively, and we
integrate it with a Transformer encoder and apply it to the task of 2D human
pose estimation. We consider different backbone architectures and pre-train
them using the DINO self-supervised learning method [3], this pre-training is
found to improve the overall prediction accuracy. We call our model BTranspose,
and experiments show that on the COCO validation set, our model achieves an AP
of 76.4, which is competitive with other methods such as [1] and has fewer
network parameters. Furthermore, we also present the dependencies of the final
predicted keypoints on both the MHSA block and the Transformer encoder layers,
providing clues on the image sub-regions the network attends to at the mid and
high levels.

### Title: WebFace260M: A Benchmark for Million-Scale Deep Face Recognition
* Paper ID: 2204.10149v1
* Paper URL: [http://arxiv.org/abs/2204.10149v1](http://arxiv.org/abs/2204.10149v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Face benchmarks empower the research community to train and evaluate
high-performance face recognition systems. In this paper, we contribute a new
million-scale recognition benchmark, containing uncurated 4M identities/260M
faces (WebFace260M) and cleaned 2M identities/42M faces (WebFace42M) training
data, as well as an elaborately designed time-constrained evaluation protocol.
Firstly, we collect 4M name lists and download 260M faces from the Internet.
Then, a Cleaning Automatically utilizing Self-Training (CAST) pipeline is
devised to purify the tremendous WebFace260M, which is efficient and scalable.
To the best of our knowledge, the cleaned WebFace42M is the largest public face
recognition training set and we expect to close the data gap between academia
and industry. Referring to practical deployments, Face Recognition Under
Inference Time conStraint (FRUITS) protocol and a new test set with rich
attributes are constructed. Besides, we gather a large-scale masked face
sub-set for biometrics assessment under COVID-19. For a comprehensive
evaluation of face matchers, three recognition tasks are performed under
standard, masked and unbiased settings, respectively. Equipped with this
benchmark, we delve into million-scale face recognition problems. A distributed
framework is developed to train face recognition models efficiently without
tampering with the performance. Enabled by WebFace42M, we reduce 40% failure
rate on the challenging IJB-C set and rank 3rd among 430 entries on NIST-FRVT.
Even 10% data (WebFace4M) shows superior performance compared with the public
training sets. Furthermore, comprehensive baselines are established under the
FRUITS-100/500/1000 milliseconds protocols. The proposed benchmark shows
enormous potential on standard, masked and unbiased face recognition scenarios.
Our WebFace260M website is https://www.face-benchmark.org.

### Title: Toward Fast, Flexible, and Robust Low-Light Image Enhancement
* Paper ID: 2204.10137v1
* Paper URL: [http://arxiv.org/abs/2204.10137v1](http://arxiv.org/abs/2204.10137v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/vis-opt-group/sci](https://github.com/vis-opt-group/sci)
* Summary: Existing low-light image enhancement techniques are mostly not only difficult
to deal with both visual quality and computational efficiency but also commonly
invalid in unknown complex scenarios. In this paper, we develop a new
Self-Calibrated Illumination (SCI) learning framework for fast, flexible, and
robust brightening images in real-world low-light scenarios. To be specific, we
establish a cascaded illumination learning process with weight sharing to
handle this task. Considering the computational burden of the cascaded pattern,
we construct the self-calibrated module which realizes the convergence between
results of each stage, producing the gains that only use the single basic block
for inference (yet has not been exploited in previous works), which drastically
diminishes computation cost. We then define the unsupervised training loss to
elevate the model capability that can adapt to general scenes. Further, we make
comprehensive explorations to excavate SCI's inherent properties (lacking in
existing works) including operation-insensitive adaptability (acquiring stable
performance under the settings of different simple operations) and
model-irrelevant generality (can be applied to illumination-based existing
works to improve performance). Finally, plenty of experiments and ablation
studies fully indicate our superiority in both quality and efficiency.
Applications on low-light face detection and nighttime semantic segmentation
fully reveal the latent practical values for SCI. The source code is available
at https://github.com/vis-opt-group/SCI.

### Title: OSSO: Obtaining Skeletal Shape from Outside
* Paper ID: 2204.10129v1
* Paper URL: [http://arxiv.org/abs/2204.10129v1](http://arxiv.org/abs/2204.10129v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/MarilynKeller/OSSO](https://github.com/MarilynKeller/OSSO)
* Summary: We address the problem of inferring the anatomic skeleton of a person, in an
arbitrary pose, from the 3D surface of the body; i.e. we predict the inside
(bones) from the outside (skin). This has many applications in medicine and
biomechanics. Existing state-of-the-art biomechanical skeletons are detailed
but do not easily generalize to new subjects. Additionally, computer vision and
graphics methods that predict skeletons are typically heuristic, not learned
from data, do not leverage the full 3D body surface, and are not validated
against ground truth. To our knowledge, our system, called OSSO (Obtaining
Skeletal Shape from Outside), is the first to learn the mapping from the 3D
body surface to the internal skeleton from real data. We do so using 1000 male
and 1000 female dual-energy X-ray absorptiometry (DXA) scans. To these, we fit
a parametric 3D body shape model (STAR) to capture the body surface and a novel
part-based 3D skeleton model to capture the bones. This provides inside/outside
training pairs. We model the statistical variation of full skeletons using PCA
in a pose-normalized space. We then train a regressor from body shape
parameters to skeleton shape parameters and refine the skeleton to satisfy
constraints on physical plausibility. Given an arbitrary 3D body shape and
pose, OSSO predicts a realistic skeleton inside. In contrast to previous work,
we evaluate the accuracy of the skeleton shape quantitatively on held-out DXA
scans, outperforming the state-of-the-art. We also show 3D skeleton prediction
from varied and challenging 3D bodies. The code to infer a skeleton from a body
shape is available for research at https://osso.is.tue.mpg.de/, and the dataset
of paired outer surface (skin) and skeleton (bone) meshes is available as a
Biobank Returned Dataset. This research has been conducted using the UK Biobank
Resource.

### Title: Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach
* Paper ID: 2204.10090v1
* Paper URL: [http://arxiv.org/abs/2204.10090v1](http://arxiv.org/abs/2204.10090v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Collecting paired training data is difficult in practice, but the unpaired
samples broadly exist. Current approaches aim at generating synthesized
training data from the unpaired samples by exploring the relationship between
the corrupted and clean data. This work proposes LUD-VAE, a deep generative
method to learn the joint probability density function from data sampled from
marginal distributions. Our approach is based on a carefully designed
probabilistic graphical model in which the clean and corrupted data domains are
conditionally independent. Using variational inference, we maximize the
evidence lower bound (ELBO) to estimate the joint probability density function.
Furthermore, we show that the ELBO is computable without paired samples under
the inference invariant assumption. This property provides the mathematical
rationale of our approach in the unpaired setting. Finally, we apply our method
to real-world image denoising and super-resolution tasks and train the models
using the synthetic data generated by the LUD-VAE. Experimental results
validate the advantages of our method over other learnable approaches.

### Title: Time Window Frechet and Metric-Based Edit Distance for Passively Collected Trajectories
* Paper ID: 2204.10053v1
* Paper URL: [http://arxiv.org/abs/2204.10053v1](http://arxiv.org/abs/2204.10053v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: The advances of modern localization techniques and the wide spread of mobile
devices have provided us great opportunities to collect and mine human mobility
trajectories. In this work, we focus on passively collected trajectories, which
are sequences of time-stamped locations that mobile entities visit. To analyse
such trajectories, a crucial part is a measure of similarity between two
trajectories. We propose the time-window Frechet distance, which enforces the
maximum temporal separation between points of two trajectories that can be
paired in the calculation of the Frechet distance, and the metric-based edit
distance which incorporates the underlying metric in the computation of the
insertion and deletion costs. Using these measures, we can cluster trajectories
to infer group motion patterns. We look at the $k$-gather problem which
requires each cluster to have at least $k$ trajectories. We prove that k-gather
remains NP-hard under edit distance, metric-based edit distance and Jaccard
distance. Finally, we improve over previous results on discrete Frechet
distance and show that there is no strongly sub-quadratic time with
approximation factor less than $1.61$ in two dimensional setting unless SETH
fails.

### Title: Understanding the Domain Gap in LiDAR Object Detection Networks
* Paper ID: 2204.10024v1
* Paper URL: [http://arxiv.org/abs/2204.10024v1](http://arxiv.org/abs/2204.10024v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: In order to make autonomous driving a reality, artificial neural networks
have to work reliably in the open-world. However, the open-world is vast and
continuously changing, so it is not technically feasible to collect and
annotate training datasets which accurately represent this domain. Therefore,
there are always domain gaps between training datasets and the open-world which
must be understood. In this work, we investigate the domain gaps between
high-resolution and low-resolution LiDAR sensors in object detection networks.
Using a unique dataset, which enables us to study sensor resolution domain gaps
independent of other effects, we show two distinct domain gaps - an inference
domain gap and a training domain gap. The inference domain gap is characterised
by a strong dependence on the number of LiDAR points per object, while the
training gap shows no such dependence. These fndings show that different
approaches are required to close these inference and training domain gaps.

### Title: Hardy spaces and quasiconformal maps in the Heisenberg group
* Paper ID: 2204.10016v1
* Paper URL: [http://arxiv.org/abs/2204.10016v1](http://arxiv.org/abs/2204.10016v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: We define Hardy spaces $H^p$, $0<p<\infty$, for quasiconformal mappings on
the Kor\'{a}nyi unit ball $B$ in the first Heisenberg group $\mathbb{H}^1$. Our
definition is stated in terms of the Heisenberg polar coordinates introduced by
Kor\'{a}nyi and Reimann, and Balogh and Tyson. First, we prove the existence of
$p_0(K)>0$ such that every $K$-quasiconformal map $f:B \to f(B) \subset
\mathbb{H}^1$ belongs to $H^p$ for all $0<p<p_0(K)$. Second, we give two
equivalent conditions for the $H^p$ membership of a quasiconformal map $f$, one
in terms of the radial limits of $f$, and one using a nontangential maximal
function of $f$. As an application, we characterize Carleson measures on $B$
via integral inequalities for quasiconformal mappings on $B$ and their radial
limits. Our paper thus extends results by Astala and Koskela, Jerison and
Weitsman, Nolder, and Zinsmeister, from $\mathbb{R}^n$ to $\mathbb{H}^1$. A
crucial difference between the proofs in $\mathbb{R}^n$ and $\mathbb{H}^1$ is
caused by the nonisotropic nature of the Kor\'{a}nyi unit sphere with its two
characteristic points.

### Title: Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach
* Paper ID: 2204.09992v1
* Paper URL: [http://arxiv.org/abs/2204.09992v1](http://arxiv.org/abs/2204.09992v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Conventional model quantization methods use a fixed quantization scheme to
different data samples, which ignores the inherent "recognition difficulty"
differences between various samples. We propose to feed different data samples
with varying quantization schemes to achieve a data-dependent dynamic
inference, at a fine-grained layer level. However, enabling this adaptive
inference with changeable layer-wise quantization schemes is challenging
because the combination of bit-widths and layers is growing exponentially,
making it extremely difficult to train a single model in such a vast searching
space and use it in practice. To solve this problem, we present the Arbitrary
Bit-width Network (ABN), where the bit-widths of a single deep network can
change at runtime for different data samples, with a layer-wise granularity.
Specifically, first we build a weight-shared layer-wise quantizable
"super-network" in which each layer can be allocated with multiple bit-widths
and thus quantized differently on demand. The super-network provides a
considerably large number of combinations of bit-widths and layers, each of
which can be used during inference without retraining or storing myriad models.
Second, based on the well-trained super-network, each layer's runtime bit-width
selection decision is modeled as a Markov Decision Process (MDP) and solved by
an adaptive inference strategy accordingly. Experiments show that the
super-network can be built without accuracy degradation, and the bit-widths
allocation of each layer can be adjusted to deal with various inputs on the
fly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement
while saving 36.2% BitOps.

### Title: CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation
* Paper ID: 2204.09914v1
* Paper URL: [http://arxiv.org/abs/2204.09914v1](http://arxiv.org/abs/2204.09914v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: LiDAR semantic segmentation essential for advanced autonomous driving is
required to be accurate, fast, and easy-deployed on mobile platforms. Previous
point-based or sparse voxel-based methods are far away from real-time
applications since time-consuming neighbor searching or sparse 3D convolution
are employed. Recent 2D projection-based methods, including range view and
multi-view fusion, can run in real time, but suffer from lower accuracy due to
information loss during the 2D projection. Besides, to improve the performance,
previous methods usually adopt test time augmentation (TTA), which further
slows down the inference process. To achieve a better speed-accuracy trade-off,
we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both
effectiveness and efficiency mainly by the following two techniques: 1) the
novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D
projected grid for efficiency, while summarizes both 2D and 3D features on 3D
point for minimal information loss; 2) the proposed transformation consistency
loss narrows the gap between the single-time model inference and TTA. The
experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the
CPGNet without ensemble models or TTA is comparable with the state-of-the-art
RPVNet, while it runs 4.7 times faster.

### Title: Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation
* Paper ID: 2204.09903v1
* Paper URL: [http://arxiv.org/abs/2204.09903v1](http://arxiv.org/abs/2204.09903v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/chunbolang/DCP](https://github.com/chunbolang/DCP)
* Summary: Few-shot segmentation, which aims to segment unseen-class objects given only
a handful of densely labeled samples, has received widespread attention from
the community. Existing approaches typically follow the prototype learning
paradigm to perform meta-inference, which fails to fully exploit the underlying
information from support image-mask pairs, resulting in various segmentation
failures, e.g., incomplete objects, ambiguous boundaries, and distractor
activation. To this end, we propose a simple yet versatile framework in the
spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is
first implemented on the annotated support image, and then the coarse
segmentation mask is divided into multiple regions with different properties.
Leveraging effective masked average pooling operations, a series of
support-induced proxies are thus derived, each playing a specific role in
conquering the above challenges. Moreover, we devise a unique parallel decoder
structure that integrates proxies with similar attributes to boost the
discrimination power. Our proposed approach, named divide-and-conquer proxies
(DCP), allows for the development of appropriate and reliable information as a
guide at the "episode" level, not just about the object cues themselves.
Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of
DCP over conventional prototype-based approaches (up to 5~10% on average),
which also establishes a new state-of-the-art. Code is available at
github.com/chunbolang/DCP.

### Title: Functional Horseshoe Smoothing for Functional Trend Estimation
* Paper ID: 2204.09898v1
* Paper URL: [http://arxiv.org/abs/2204.09898v1](http://arxiv.org/abs/2204.09898v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Due to developments in instruments and computers, functional observations are
increasingly popular. However, effective methodologies for flexibly estimating
the underlying trends with valid uncertainty quantification for a sequence of
functional data (e.g. functional time series) are still scarce. In this work,
we develop a locally adaptive smoothing method, called functional horseshoe
smoothing, by introducing a shrinkage prior to the general order of differences
of functional variables. This allows us to capture abrupt changes by taking
advantage of the shrinkage capability and also to assess uncertainty by
Bayesian inference. The fully Bayesian framework also allows the selection of
the number of basis functions via the posterior predictive loss. Also, by
taking advantage of the nature of functional data, this method is able to
handle heterogeneously observed data without data augmentation. We show the
theoretical properties of the proposed prior distribution and the posterior
mean, and finally demonstrate them through simulation studies and applications
to a real-world dataset.

### Title: Non-autoregressive Model for Full-line Code Completion
* Paper ID: 2204.09877v1
* Paper URL: [http://arxiv.org/abs/2204.09877v1](http://arxiv.org/abs/2204.09877v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Code completion tools are frequently used by software developers to
accelerate software development by suggesting the following code elements.
Completing a sequence of code tokens (e.g., a full line of code) has been
proved more efficient than predicting a single token at a time. To complete the
code sequence, researchers are employing AutoRegressive (AR) decoders to
generate tokens in a left-to-right, token-by-token fashion. Consequently, the
prediction of the next token depends on all previously generated tokens, which
leads to high latency in inference. To improve the efficiency and accuracy of
full-line code completion, in this paper, we propose a Non-AutoRegressive (NAR)
model for code completion boosted by a syntax-aware sampling strategy. Our
experimental results on two widely used datasets suggest that our model
outperforms both AR and NAR baselines on full-line code completion, and it is
faster than the AR model with up to 9 times speed-up.

### Title: Gaussian Processes for real-time 3D motion and uncertainty estimation during MR-guided radiotherapy
* Paper ID: 2204.09873v1
* Paper URL: [http://arxiv.org/abs/2204.09873v1](http://arxiv.org/abs/2204.09873v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Respiratory motion during radiotherapy causes uncertainty in the tumor's
location, which is typically addressed by an increased radiation area and a
decreased dose. As a result, the treatments' efficacy is reduced. The recently
proposed hybrid MR-linac scanner holds the promise to efficiently deal with
such respiratory motion through real-time adaptive MR-guided radiotherapy
(MRgRT). For MRgRT, motion-fields should be estimated from MR-data and the
radiotherapy plan should be adapted in real-time according to the estimated
motion-fields. All of this should be performed with a total latency of
maximally 200 ms, including data acquisition and reconstruction. A measure of
confidence in such estimated motion-fields is highly desirable, for instance to
ensure the patient's safety in case of unexpected and undesirable motion. In
this work, we propose a framework based on Gaussian Processes to infer 3D
motion-fields and uncertainty maps in real-time from only three readouts of
MR-data. We demonstrated an inference frame rate up to 69 Hz including data
acquisition and reconstruction, thereby exploiting the limited amount of
required MR-data. Additionally, we designed a rejection criterion based on the
motion-field uncertainty maps to demonstrate the framework's potential for
quality assurance. The framework was validated in silico and in vivo on healthy
volunteer data (n=5) acquired using an MR-linac, thereby taking into account
different breathing patterns and controlled bulk motion. Results indicate
end-point-errors with a 75th percentile below 1mm in silico, and a correct
detection of erroneous motion estimates with the rejection criterion.
Altogether, the results show the potential of the framework for application in
real-time MR-guided radiotherapy with an MR-linac.

### Title: The $Î¸$-augmented model for Bayesian semiparametric inference on functional parameters
* Paper ID: 2204.09862v1
* Paper URL: [http://arxiv.org/abs/2204.09862v1](http://arxiv.org/abs/2204.09862v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Semiparametric Bayesian inference has so far relied on models for the
observable that partition into two parts, one being parametric and the other
nonparametric, with the target parameter being dependent on the parametric
component. While a partitioned structure makes specification of the marginal
prior on the target parameter simple to perform, it often arises from
conditional modelling which is subject to misspecification and ultimately a
lack of consistency. We introduce a new type of semiparametric model to allow
easy prior specification for a parameter that is defined as a functional of the
distribution for the observable. Our semiparametric model is obtained as an
extension of nonparametric models that are consistent under very general
conditions. This type of Bayesian semiparametric model can be used to obtain
Bayesian versions of Frequentist estimators that are defined as functionals of
the empirical distribution. This gives us new opportunities to conduct Bayesian
analysis in problems where Frequentist estimators exist but not well-accepted
likelihoods.

### Title: Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and Local Information
* Paper ID: 2204.09860v1
* Paper URL: [http://arxiv.org/abs/2204.09860v1](http://arxiv.org/abs/2204.09860v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/xiaoyuan1996/galr](https://github.com/xiaoyuan1996/galr)
* Summary: Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become
an urgent research hotspot due to its ability of enabling fast and flexible
information extraction on remote sensing (RS) images. However, current RSCTIR
methods mainly focus on global features of RS images, which leads to the
neglect of local features that reflect target relationships and saliency. In
this article, we first propose a novel RSCTIR framework based on global and
local information (GaLR), and design a multi-level information dynamic fusion
(MIDF) module to efficaciously integrate features of different levels. MIDF
leverages local information to correct global information, utilizes global
information to supplement local information, and uses the dynamic addition of
the two to generate prominent visual representation. To alleviate the pressure
of the redundant targets on the graph convolution network (GCN) and to improve
the model s attention on salient instances during modeling local features, the
de-noised representation matrix and the enhanced adjacency matrix (DREA) are
devised to assist GCN in producing superior local representations. DREA not
only filters out redundant features with high similarity, but also obtains more
powerful local features by enhancing the features of prominent objects.
Finally, to make full use of the information in the similarity matrix during
inference, we come up with a plug-and-play multivariate rerank (MR) algorithm.
The algorithm utilizes the k nearest neighbors of the retrieval results to
perform a reverse search, and improves the performance by combining multiple
components of bidirectional retrieval. Extensive experiments on public datasets
strongly demonstrate the state-of-the-art performance of GaLR methods on the
RSCTIR task. The code of GaLR method, MR algorithm, and corresponding files
have been made available at https://github.com/xiaoyuan1996/GaLR .

### Title: A Masked Image Reconstruction Network for Document-level Relation Extraction
* Paper ID: 2204.09851v1
* Paper URL: [http://arxiv.org/abs/2204.09851v1](http://arxiv.org/abs/2204.09851v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Document-level relation extraction aims to extract relations among entities
within a document. Compared with its sentence-level counterpart, Document-level
relation extraction requires inference over multiple sentences to extract
complex relational triples. Previous research normally complete reasoning
through information propagation on the mention-level or entity-level
document-graphs, regardless of the correlations between the relationships. In
this paper, we propose a novel Document-level Relation Extraction model based
on a Masked Image Reconstruction network (DRE-MIR), which models inference as a
masked image reconstruction problem to capture the correlations between
relationships. Specifically, we first leverage an encoder module to get the
features of entities and construct the entity-pair matrix based on the
features. After that, we look on the entity-pair matrix as an image and then
randomly mask it and restore it through an inference module to capture the
correlations between the relationships. We evaluate our model on three public
document-level relation extraction datasets, i.e. DocRED, CDR, and GDA.
Experimental results demonstrate that our model achieves state-of-the-art
performance on these three datasets and has excellent robustness against the
noises during the inference process.

### Title: FedCL: Federated Contrastive Learning for Privacy-Preserving Recommendation
* Paper ID: 2204.09850v1
* Paper URL: [http://arxiv.org/abs/2204.09850v1](http://arxiv.org/abs/2204.09850v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Contrastive learning is widely used for recommendation model learning, where
selecting representative and informative negative samples is critical. Existing
methods usually focus on centralized data, where abundant and high-quality
negative samples are easy to obtain. However, centralized user data storage and
exploitation may lead to privacy risks and concerns, while decentralized user
data on a single client can be too sparse and biased for accurate contrastive
learning. In this paper, we propose a federated contrastive learning method
named FedCL for privacy-preserving recommendation, which can exploit
high-quality negative samples for effective model training with privacy well
protected. We first infer user embeddings from local user data through the
local model on each client, and then perturb them with local differential
privacy (LDP) before sending them to a central server for hard negative
sampling. Since individual user embedding contains heavy noise due to LDP, we
propose to cluster user embeddings on the server to mitigate the influence of
noise, and the cluster centroids are used to retrieve hard negative samples
from the item pool. These hard negative samples are delivered to user clients
and mixed with the observed negative samples from local data as well as
in-batch negatives constructed from positive samples for federated model
training. Extensive experiments on four benchmark datasets show FedCL can
empower various recommendation methods in a privacy-preserving way.

### Title: 6GAN: IPv6 Multi-Pattern Target Generation via Generative Adversarial Nets with Reinforcement Learning
* Paper ID: 2204.09839v1
* Paper URL: [http://arxiv.org/abs/2204.09839v1](http://arxiv.org/abs/2204.09839v1)
* Updated Date: 2022-04-21
* Code URL: [https://github.com/cuitianyu961030/6gan](https://github.com/cuitianyu961030/6gan)
* Summary: Global IPv6 scanning has always been a challenge for researchers because of
the limited network speed and computational power. Target generation algorithms
are recently proposed to overcome the problem for Internet assessments by
predicting a candidate set to scan. However, IPv6 custom address configuration
emerges diverse addressing patterns discouraging algorithmic inference.
Widespread IPv6 alias could also mislead the algorithm to discover aliased
regions rather than valid host targets. In this paper, we introduce 6GAN, a
novel architecture built with Generative Adversarial Net (GAN) and
reinforcement learning for multi-pattern target generation. 6GAN forces
multiple generators to train with a multi-class discriminator and an alias
detector to generate non-aliased active targets with different addressing
pattern types. The rewards from the discriminator and the alias detector help
supervise the address sequence decision-making process. After adversarial
training, 6GAN's generators could keep a strong imitating ability for each
pattern and 6GAN's discriminator obtains outstanding pattern discrimination
ability with a 0.966 accuracy. Experiments indicate that our work outperformed
the state-of-the-art target generation algorithms by reaching a higher-quality
candidate set.

### Title: Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing
* Paper ID: 2204.09817v1
* Paper URL: [http://arxiv.org/abs/2204.09817v1](http://arxiv.org/abs/2204.09817v1)
* Updated Date: 2022-04-21
* Code URL: null
* Summary: Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.

